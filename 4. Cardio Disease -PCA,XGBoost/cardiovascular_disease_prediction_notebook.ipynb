{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M27qF7CTrBqc"
   },
   "source": [
    "# TASK #1 : UNDERSTAND THE PROBLEM STATEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNl52nl3qiyL"
   },
   "source": [
    "\n",
    "Aim of the problem is to detect the presence or absence of cardiovascular disease in person based on the given features.\n",
    "Features available are:\n",
    "\n",
    "\n",
    "- Age | Objective Feature | age | int (days)\n",
    "- Height | Objective Feature | height | int (cm) |\n",
    "- Weight | Objective Feature | weight | float (kg) |\n",
    "- Gender | Objective Feature | gender | categorical code |\n",
    "- Systolic blood pressure | Examination Feature | ap_hi | int |\n",
    "- Diastolic blood pressure | Examination Feature | ap_lo | int |\n",
    "- Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n",
    "- Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\n",
    "- Smoking | Subjective Feature | smoke | binary |\n",
    "- Alcohol intake | Subjective Feature | alco | binary |\n",
    "- Physical activity | Subjective Feature | active | binary |\n",
    "- Presence or absence of cardiovascular disease | Target Variable | cardio | binary |\n",
    "\n",
    "Note that:\n",
    "- Objective: factual information;\n",
    "- Examination: results of medical examination;\n",
    "- Subjective: information given by the patient.\n",
    "\n",
    "Data Source:https://www.kaggle.com/sulianova/cardiovascular-disease-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK #2: IMPORT LIBRARIES AND DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "S0Cx3743urFY",
    "outputId": "b820039b-7ccb-4a68-8206-77bae97680dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjIiJdM4u1IE"
   },
   "outputs": [],
   "source": [
    "# read the csv file \n",
    "cardio_df = pd.read_csv(\"cardio_train.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "q4_wPDKCu5Uc",
    "outputId": "886d2aaf-0205-4f46-96a7-629d0f367d2f"
   },
   "outputs": [],
   "source": [
    "cardio_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMcr7xqMQre2"
   },
   "source": [
    "# TASK #3: PERFORM EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop id\n",
    "\n",
    "cardio_df = cardio_df.drop(columns = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the age is given in days, we convert it into years\n",
    "\n",
    "cardio_df['age'] = cardio_df['age']/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "6JYkJo9pYaet",
    "outputId": "fcc89a98-b0a9-499d-acde-e480e78dccfc"
   },
   "outputs": [],
   "source": [
    "# checking the null values\n",
    "cardio_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "hMq3-KWOx0e1",
    "outputId": "22a5b184-1f07-46ef-dfc1-f8377fd7042f"
   },
   "outputs": [],
   "source": [
    "# Checking the dataframe information\n",
    "\n",
    "cardio_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "Nn1Oxk2SzPX3",
    "outputId": "95f0265a-5e75-4a32-d771-4b3d15850c3c"
   },
   "outputs": [],
   "source": [
    "# Statistical summary of the dataframe\n",
    "cardio_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI CHALLENGE\n",
    "- Obtain the features of the individuals who are older than 64.8 years old "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK #4: VISUALIZE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI CHALLENGE\n",
    "- plot the histogram for all features (use 20 bins) \n",
    "- plot the correlation matrix and indicate if there exists any correlations between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ka9uFRXSkWHw",
    "outputId": "f42a681e-93d4-4b1f-a29c-f58fc8a6f974"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(cardio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "9q-tFxvskWDa",
    "outputId": "8834e9ec-7676-4e86-c5e7-20f4e9eccbcb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 932
    },
    "colab_type": "code",
    "id": "clmblKfur8rm",
    "outputId": "760b0882-8993-4aed-9c38-866ef8a7c6cd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53qDZFRn3-S1"
   },
   "source": [
    "# TASK #5: CREATE TRAINING AND TESTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into target and features\n",
    "\n",
    "df_target = cardio_df['cardio']\n",
    "df_final = cardio_df.drop(columns =['cardio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8QQTkOBL1yUR",
    "outputId": "e79771c7-a34d-430f-db8a-fa4c1253af78"
   },
   "outputs": [],
   "source": [
    "cardio_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XpGU63Ne1e9P",
    "outputId": "e16c74ca-dc1c-416c-dc44-7f927bb99bc6"
   },
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OjGj0RALA0qZ",
    "outputId": "26559a6c-880b-45b4-a1e8-3c4b92bea889"
   },
   "outputs": [],
   "source": [
    "df_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GoReLFfnA6uF"
   },
   "outputs": [],
   "source": [
    "#spliting the data in to test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final, df_target, test_size = 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK #6: TRAIN AND TEST XGBOOST MODEL IN LOCAL MODE (NOTE THAT SAGEMAKER BUILT-IN ALGORITHMS ARE NOT USED HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install xgboost\n",
    "\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yHL-6mKwBURs",
    "outputId": "10d71b6d-9c2b-4bab-8b27-d3c5883e6a25"
   },
   "outputs": [],
   "source": [
    "# use xgboost model in local mode\n",
    "\n",
    "# note that we have not performed any normalization or scaling since XGBoost is not sensitive to this.\n",
    "# XGboost is a type of ensemble algorithms and works by selecting thresholds or cut points on features to split a node. \n",
    "# It doesn't really matter if the features are scaled or not.\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# model = XGBClassifier(learning_rate=0.01, n_estimators=100, objective='binary:logistic')\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "\n",
    "predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess trained model performance on training dataset\n",
    "predict_train = model.predict(X_train)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_train, predict_train)\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics for training dataset\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_train, predict_train)))\n",
    "print(\"Recall = {}\".format(recall_score(y_train, predict_train)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_train, predict_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics for testing dataset\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, predict)))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, predict)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predict)\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI CHALLENGE\n",
    " - Retrain the XGBoost algorithm with large number of estimators (n_estimators = 500) and more depth (max_depth = 20)\n",
    " - Comment on the accuracy for both the training and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI CHALLENGE\n",
    "- Attempt to tune the XGBoost classifier using gridsearch, start with the param_grid listed below\n",
    "- Check out these great resources: https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'gamma': [0.5, 1, 5],   # regularization parameter \n",
    "        'subsample': [0.6, 0.8, 1.0], # % of rows taken to build each tree\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0], # number of columns used by each tree\n",
    "        'max_depth': [3, 4, 5] # depth of each tree\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_optim = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics for testing dataset\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, y_predict_optim)))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, y_predict_optim)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, y_predict_optim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK #7: PERFORM DIMENSIONALITY REDUCTION USING PCA ( USING SAGEMAKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python\n",
    "# Boto3 allows Python developer to write software that makes use of services like Amazon S3 and Amazon EC2\n",
    "\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Let's create a Sagemaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Let's define the S3 bucket and prefix that we want to use in this session\n",
    "bucket = 'sagemaker-practical'  # Use the default bucket name\n",
    "prefix = 'pca'  # prefix is the subfolder within the bucket.\n",
    "\n",
    "#Let's get the execution role for the notebook instance. \n",
    "# This is the IAM role that you created when you created your notebook instance. You pass the role to the training job.\n",
    "# Note that AWS Identity and Access Management (IAM) role that Amazon SageMaker can assume to perform tasks on your behalf (for example, reading training results, called model artifacts, from the S3 bucket and writing training results to Amazon S3). \n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io # The io module allows for dealing with various types of I/O (text I/O, binary I/O and raw I/O). \n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac # sagemaker common libary\n",
    "\n",
    "# Code below converts the data in numpy array format to RecordIO format\n",
    "# This is the format required by Sagemaker PCA\n",
    "\n",
    "buf = io.BytesIO() # create an in-memory byte array (buf is a buffer I will be writing to)\n",
    "df_matrix = df_final.to_numpy() # convert the dataframe into 2-dimensional array\n",
    "smac.write_numpy_to_dense_tensor(buf, df_matrix)\n",
    "buf.seek(0)\n",
    "\n",
    "# When you write to in-memory byte arrays, it increments 1 every time you write to it\n",
    "# Let's reset that back to zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Code to upload RecordIO data to S3\n",
    " \n",
    "# Key refers to the name of the file \n",
    " \n",
    "key = 'pca'\n",
    "\n",
    "#following code uploads the data in record-io format to S3 bucket to be accessed later for training\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "\n",
    "# Let's print out the training data location in s3\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "\n",
    "\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output placeholder in S3 bucket to store the PCA output\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to get the training container of sagemaker built-in algorithms\n",
    "# all we have to do is to specify the name of the algorithm, that we want to use\n",
    "\n",
    "# Let's obtain a reference to the pca container image\n",
    "# Note that all  models are named estimators\n",
    "# You don't have to specify (hardcode) the region, get_image_uri will get the current region name using boto3.Session\n",
    "\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have pass in the container, the type of instance that we would like to use for training \n",
    "# output path and sagemaker session into the Estimator. \n",
    "# We can also specify how many instances we would like to use for training\n",
    "\n",
    "\n",
    "pca = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sagemaker_session)\n",
    "\n",
    "# We can tune parameters like the number of features that we are passing in, mode of algorithm, mini batch size and number of pca components\n",
    "\n",
    "\n",
    "pca.set_hyperparameters(feature_dim=11,\n",
    "                        num_components=6,\n",
    "                        subtract_mean=False,\n",
    "                        algorithm_mode='regular',\n",
    "                        mini_batch_size=100)\n",
    "\n",
    "\n",
    "# Pass in the training data from S3 to train the pca model\n",
    "\n",
    "\n",
    "pca.fit({'train': s3_train_data})\n",
    "\n",
    "# Let's see the progress using cloudwatch logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI CHALLENGE:\n",
    " - Retrain the model with the following number of components 5, 4, and 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK #8: DEPLOY THE TRAINED PCA MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to perform inference \n",
    "\n",
    "pca_reduction = pca.deploy(initial_instance_count = 1,\n",
    "                                          instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "\n",
    "# Content type overrides the data that will be passed to the deployed model, since the deployed model expects data in text/csv format.\n",
    "\n",
    "# Serializer accepts a single argument, the input data, and returns a sequence of bytes in the specified content type\n",
    "\n",
    "# Deserializer accepts two arguments, the result data and the response content type, and return a sequence of bytes in the specified content type.\n",
    "\n",
    "# Reference: https://sagemaker.readthedocs.io/en/stable/predictors.html\n",
    "\n",
    "\n",
    "pca_reduction.content_type = 'text/csv'\n",
    "pca_reduction.serializer = csv_serializer\n",
    "pca_reduction.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on the test data\n",
    "\n",
    "result = pca_reduction.predict(np.array(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result # results are in Json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the results are in Json format, we access the scores by iterating through the scores in the predictions\n",
    "predictions = np.array([r['projection'] for r in result['projections']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the end-point\n",
    "\n",
    "pca_reduction.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK #9: TRAIN AND EVALUATE XGBOOST MODEL ON DATA AFTER DIMENSIONALITY REDUCTION (USING SAGEMAKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array into dataframe in a way that target variable is set as the first column and is followed by feature columns\n",
    "# This is because sagemaker built-in algorithm expects the data in this format\n",
    "\n",
    "train_data = pd.DataFrame({'Target':df_target})\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(predictions.shape[1]):\n",
    "    train_data[i] = predictions[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = int(0.9 * train_data.shape[0])\n",
    "train_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data in dataframe and then split the dataframe into train, test and validation sets.\n",
    "\n",
    "import sklearn \n",
    "\n",
    "train_data = sklearn.utils.shuffle(train_data)\n",
    "train, test, valid = train_data[:train_data_size], train_data[train_data_size:train_data_size + 3500], train_data[train_data_size + 3500:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape,valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = test.drop(columns = ['Target']), test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_data and validation_data as csv files\n",
    "\n",
    "train.to_csv('train.csv',header = False, index = False)\n",
    "valid.to_csv('valid.csv',header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'XGBoost-Classifier'\n",
    "key = 'XGBoost-Classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from csv file and then upload the data to s3 bucket\n",
    "with open('train.csv','rb') as f:\n",
    "    # The following code uploads the data into S3 bucket to be accessed later for training\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(f)\n",
    "\n",
    "# Let's print out the training data location in s3\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data from csv file and then upload the data to s3 bucket\n",
    "with open('valid.csv','rb') as f:\n",
    "    # The following code uploads the data into S3 bucket to be accessed later for training\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'valid', key)).upload_fileobj(f)\n",
    "\n",
    "# Let's print out the validation data location in s3\n",
    "s3_valid_data = 's3://{}/{}/valid/{}'.format(bucket, prefix, key)\n",
    "print('uploaded validation data location: {}'.format(s3_valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates output placeholder in S3 bucket to store the linear learner output\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to get the training container of sagemaker built-in algorithms\n",
    "# all we have to do is to specify the name of the algorithm, that we want to use\n",
    "\n",
    "# Let's obtain a reference to the XGBoost container image\n",
    "# Note that all  models are named estimators\n",
    "# You don't have to specify (hardcode) the region, get_image_uri will get the current region name using boto3.Session\n",
    "\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost','0.90-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have pass in the container, the type of instance that we would like to use for training \n",
    "# output path and sagemaker session into the Estimator. \n",
    "# We can also specify how many instances we would like to use for training\n",
    "\n",
    "\n",
    "Xgboost_classifier = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.m4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sagemaker_session)\n",
    "\n",
    "# To improve the performance of the model, a hyperparameters tuning job need to be run \n",
    "\n",
    "Xgboost_classifier.set_hyperparameters(max_depth=3,\n",
    "                           objective='multi:softmax',\n",
    "                           num_class= 2,\n",
    "                           eta = 0.5,\n",
    "                           num_round = 150\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"train\", \"validation\" channels to feed in the model\n",
    "# Source: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "\n",
    "\n",
    "train_input = sagemaker.session.s3_input(s3_data = s3_train_data, content_type='csv',s3_data_type = 'S3Prefix')\n",
    "valid_input = sagemaker.session.s3_input(s3_data = s3_valid_data, content_type='csv',s3_data_type = 'S3Prefix')\n",
    "\n",
    "Xgboost_classifier.fit({'train': train_input, 'validation': valid_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI CHALLENGE\n",
    "- Retrain the XGBoost model with deeper trees (max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK #10: DEPLOY AND TEST THE TRAINED XGBOOST MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to perfrom inference \n",
    "\n",
    "Xgboost_classifier = Xgboost_classifier.deploy(initial_instance_count = 1,\n",
    "                                          instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content type over-rides the data that will be passed to the deployed model, since the deployed model expects data in text/csv format, we specify this as content -type.\n",
    "# Serializer accepts a single argument, the input data, and returns a sequence of bytes in the specified content type\n",
    "#Reference: https://sagemaker.readthedocs.io/en/stable/predictors.html\n",
    "\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "Xgboost_classifier.content_type = 'text/csv'\n",
    "Xgboost_classifier.serializer = csv_serializer\n",
    "Xgboost_classifier.deserializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "\n",
    "XGB_prediction = Xgboost_classifier.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom code to convert the values in bytes format to array\n",
    "\n",
    "def bytes_2_array(x):\n",
    "    \n",
    "    #makes entire prediction as string and splits based on ','\n",
    "    l = str(x).split(',')\n",
    "    \n",
    "    #Since the first element contains unwanted characters like (b,',') we remove them\n",
    "    l[0] = l[0][2:]\n",
    "    #same-thing as above remove the unwanted last character (')\n",
    "    l[-1] = l[-1][:-1]\n",
    "    \n",
    "    #iterating through the list of strings and converting them into float type\n",
    "    for i in range(len(l)):\n",
    "        l[i] = float(l[i])\n",
    "        \n",
    "    #converting the list to into array\n",
    "    l = np.array(l).astype('float32')\n",
    "    \n",
    "    #reshape one-dimensional array to two-dimentaional array\n",
    "    return l.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = bytes_2_array(XGB_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, predicted_values, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, predicted_values, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, predicted_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predicted_values)\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the end-point\n",
    "\n",
    "Xgboost_classifier.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXCELLENT JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI CHALLENGE SOLUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_df [ cardio_df['ap_hi'] == 16020]\n",
    "cardio_df [ cardio_df['age'] > 64.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_df.hist(bins = 30, figsize = (20,20), color = 'r')\n",
    "# get the correlation matrix\n",
    "\n",
    "corr_matrix = cardio_df.corr()\n",
    "corr_matrix\n",
    "# plotting the correlation matrix\n",
    "plt.figure(figsize = (16,16))\n",
    "sns.heatmap(corr_matrix, annot = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(learning_rate=0.01, n_estimators=100, objective='binary:logistic')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(xgb_model, param_grid, refit = True, verbose = 4)\n",
    "grid.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Graduate_Admission_Prediction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
