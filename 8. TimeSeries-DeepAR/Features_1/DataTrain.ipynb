{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR Model - Predict Bike Rental with Dynamic Features\n",
    "\n",
    "Note: This dataset is not a true timeseries as there a lot of gaps\n",
    "\n",
    "We have data only for first 20 days of each month and model needs to predict the rentals for \n",
    "the remaining days of the month. The dataset consists of two years data. DeepAR will shine with true multiple-timeseries dataset like the electricity example given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# This code is derived from AWS SageMaker Samples:\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_categories = False\n",
    "# Set a good base job name\n",
    "# It will help in identifying trained models and endpoints\n",
    "base_job_name = 'deepar-AAPL-with-dynamic-feat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'mw-ml-sagemaker'\n",
    "prefix = 'deepar/bikerental'\n",
    "\n",
    "# This structure allows multiple training and test files for model development and testing\n",
    "s3_data_path = \"{}/{}/data_dynamic\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mw-ml-sagemaker/deepar/bikerental/data_dynamic',\n",
       " 'mw-ml-sagemaker/deepar/bikerental/output')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path,s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name is referred as key name in S3\n",
    "# Files stored in S3 are automatically replicated across\n",
    "# three different availability zones in the region where the bucket was created.\n",
    "# http://boto3.readthedocs.io/en/latest/guide/s3.html\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload one or more training files and test files to S3\n",
    "write_to_s3('train_dynamic_feat.json',bucket,'deepar/bikerental/data_dynamic/train/train_dynamic_feat.json')\n",
    "write_to_s3('test_dynamic_feat.json',bucket,'deepar/bikerental/data_dynamic/test/test_dynamic_feat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a session with AWS\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::480536818350:role/service-role/AmazonSageMaker-ExecutionRole-20201001T191047\n"
     ]
    }
   ],
   "source": [
    "# This role contains the permissions needed to train, deploy models\n",
    "# SageMaker Service is trusted to assume this role\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DeepAR Container 522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "# https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html#sagemaker.image_uris.retrieve\n",
    "\n",
    "# SDK 2 uses image_uris.retrieve the container image location\n",
    "\n",
    "# Use DeepAR Container\n",
    "container = sagemaker.image_uris.retrieve(\"forecasting-deepar\",sess.boto_region_name)\n",
    "\n",
    "print (f'Using DeepAR Container {container}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='D' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 5\n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check Free Tier (if you are still under free-tier)\n",
    "# At this time, m4.xlarge is offered as part of 2 months free tier\n",
    "# https://aws.amazon.com/sagemaker/pricing/\n",
    "# If you are outside of free-tier, you can also use ml.m5.xlarge  (newer generation instance)\n",
    "# In this example, I am using ml.m5.xlarge for training\n",
    "\n",
    "# Dynamic Feat - Using a large instance ml.c5.4xlarge = 16 CPU, 32 GB\n",
    "# 'ml.c4.xlarge' -> 'ml.c5.4xlarge'. out of memory error with c4.xlarge\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.4xlarge',\n",
    "    base_job_name=base_job_name,\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training job\n",
    "# Specify type and number of instances to use\n",
    "#   Reference: http://sagemaker.readthedocs.io/en/latest/estimators.html\n",
    "# SDK 2.x version does not require train prefix for instance count and type\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=\"s3://\" + s3_output_path,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name=base_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D', 5, 5)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_freq': 'D',\n",
       " 'epochs': '400',\n",
       " 'early_stopping_patience': '10',\n",
       " 'mini_batch_size': '64',\n",
       " 'learning_rate': '5E-4',\n",
       " 'context_length': '5',\n",
       " 'prediction_length': '5',\n",
       " 'cardinality': ''}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are simply referring to train path and test path\n",
    "# You can have multiple files in each path\n",
    "# SageMaker will use all the files\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://mw-ml-sagemaker/deepar/bikerental/data_dynamic/train/',\n",
       " 'test': 's3://mw-ml-sagemaker/deepar/bikerental/data_dynamic/test/'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-31 22:20:03 Starting - Starting the training job...\n",
      "2020-12-31 22:20:26 Starting - Launching requested ML instancesProfilerReport-1609453203: InProgress\n",
      "......\n",
      "2020-12-31 22:21:27 Starting - Preparing the instances for training......\n",
      "2020-12-31 22:22:29 Downloading - Downloading input data\n",
      "2020-12-31 22:22:29 Training - Downloading the training image...\n",
      "2020-12-31 22:22:48 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:49 INFO 140699746432832] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:49 INFO 140699746432832] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'5', u'epochs': u'400', u'time_freq': u'D', u'context_length': u'5', u'mini_batch_size': u'64', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:49 INFO 140699746432832] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'5', u'time_freq': u'D', u'context_length': u'5', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:49 INFO 140699746432832] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=4 from dataset.\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Training set statistics:\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Real time series\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] number of time series: 1\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] number of observations: 361\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] mean target length: 361\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] min/mean/max target: 53.0979995728/93.0703233206/134.179992676\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] mean abs(target): 93.0703233206\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] contains missing values: no\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Small number of time series. Doing 640 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Test set statistics:\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Real time series\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] number of time series: 1\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] number of observations: 366\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] mean target length: 366\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] min/mean/max target: 53.0979995728/93.5431181694/134.179992676\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] mean abs(target): 93.5431181694\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] contains missing values: no\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] nvidia-smi took: 0.0251560211182 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 23.382902145385742, \"sum\": 23.382902145385742, \"min\": 23.382902145385742}}, \"EndTime\": 1609453370.100755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453370.076447}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 69.90194320678711, \"sum\": 69.90194320678711, \"min\": 69.90194320678711}}, \"EndTime\": 1609453370.146498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453370.100831}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Epoch[0] Batch[0] avg_epoch_loss=7.077779\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=7.07777881622\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Epoch[0] Batch[5] avg_epoch_loss=6.906337\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=6.90633662542\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Epoch[0] Batch [5]#011Speed: 3533.64 samples/sec#011loss=6.906337\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 487.61796951293945, \"sum\": 487.61796951293945, \"min\": 487.61796951293945}}, \"EndTime\": 1609453370.634259, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453370.146554}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1260.79618677 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] #quality_metric: host=algo-1, epoch=0, train loss <loss>=6.78817639351\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_0e0f55f5-ce90-41c9-b5c7-6fc9286c159d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.248878479003906, \"sum\": 7.248878479003906, \"min\": 7.248878479003906}}, \"EndTime\": 1609453370.642184, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453370.634394}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Epoch[1] Batch[0] avg_epoch_loss=6.480184\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=6.48018407822\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Epoch[1] Batch[5] avg_epoch_loss=6.358708\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=6.35870774587\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:50 INFO 140699746432832] Epoch[1] Batch [5]#011Speed: 3624.95 samples/sec#011loss=6.358708\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 424.6630668640137, \"sum\": 424.6630668640137, \"min\": 424.6630668640137}}, \"EndTime\": 1609453371.066961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453370.642243}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1424.33019735 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #quality_metric: host=algo-1, epoch=1, train loss <loss>=6.27492284775\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_999bab45-477d-402c-8866-3178ea32b910-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.441925048828125, \"sum\": 8.441925048828125, \"min\": 8.441925048828125}}, \"EndTime\": 1609453371.075854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453371.067029}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Epoch[2] Batch[0] avg_epoch_loss=6.134584\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=6.13458442688\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Epoch[2] Batch[5] avg_epoch_loss=6.040968\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=6.04096810023\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Epoch[2] Batch [5]#011Speed: 3669.01 samples/sec#011loss=6.040968\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 458.77885818481445, \"sum\": 458.77885818481445, \"min\": 458.77885818481445}}, \"EndTime\": 1609453371.534754, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453371.075916}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1320.6157651 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #quality_metric: host=algo-1, epoch=2, train loss <loss>=5.96809635162\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_b767dd52-273e-4b52-9bd2-f7ae8f0219d1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.167173385620117, \"sum\": 6.167173385620117, \"min\": 6.167173385620117}}, \"EndTime\": 1609453371.541528, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453371.534819}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Epoch[3] Batch[0] avg_epoch_loss=5.797288\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=5.79728841782\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Epoch[3] Batch[5] avg_epoch_loss=5.715120\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=5.71511999766\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Epoch[3] Batch [5]#011Speed: 3982.08 samples/sec#011loss=5.715120\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 404.2220115661621, \"sum\": 404.2220115661621, \"min\": 404.2220115661621}}, \"EndTime\": 1609453371.945868, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453371.541591}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1545.80529661 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] #quality_metric: host=algo-1, epoch=3, train loss <loss>=5.6708468914\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:51 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_74d0aba9-deda-47c1-81a9-ab0fcb9b9c42-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.510019302368164, \"sum\": 6.510019302368164, \"min\": 6.510019302368164}}, \"EndTime\": 1609453371.952953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453371.945934}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] Epoch[4] Batch[0] avg_epoch_loss=5.493305\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=5.49330472946\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] Epoch[4] Batch[5] avg_epoch_loss=5.470321\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=5.47032062213\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] Epoch[4] Batch [5]#011Speed: 3550.00 samples/sec#011loss=5.470321\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 429.7299385070801, \"sum\": 429.7299385070801, \"min\": 429.7299385070801}}, \"EndTime\": 1609453372.382783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453371.953002}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1440.14072394 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #quality_metric: host=algo-1, epoch=4, train loss <loss>=5.42600960732\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_05aa1e49-5118-4d97-b13e-99b3f43c07d4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.556034088134766, \"sum\": 6.556034088134766, \"min\": 6.556034088134766}}, \"EndTime\": 1609453372.389844, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453372.382844}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] Epoch[5] Batch[0] avg_epoch_loss=5.334620\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=5.33461999893\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] Epoch[5] Batch[5] avg_epoch_loss=5.267998\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=5.26799805959\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] Epoch[5] Batch [5]#011Speed: 4027.08 samples/sec#011loss=5.267998\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 409.32393074035645, \"sum\": 409.32393074035645, \"min\": 409.32393074035645}}, \"EndTime\": 1609453372.799272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453372.389894}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1538.75045203 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] #quality_metric: host=algo-1, epoch=5, train loss <loss>=5.2303311348\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:52 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_53a874d4-c30a-494d-a29c-bf15a427787c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.5059661865234375, \"sum\": 6.5059661865234375, \"min\": 6.5059661865234375}}, \"EndTime\": 1609453372.806296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453372.799339}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Epoch[6] Batch[0] avg_epoch_loss=5.062069\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=5.06206941605\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Epoch[6] Batch[5] avg_epoch_loss=4.992602\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=4.99260171254\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Epoch[6] Batch [5]#011Speed: 4018.23 samples/sec#011loss=4.992602\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Epoch[6] Batch[10] avg_epoch_loss=4.946546\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=4.89127874374\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Epoch[6] Batch [10]#011Speed: 3526.56 samples/sec#011loss=4.891279\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 441.01786613464355, \"sum\": 441.01786613464355, \"min\": 441.01786613464355}}, \"EndTime\": 1609453373.247424, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453372.806351}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1552.89009287 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #quality_metric: host=algo-1, epoch=6, train loss <loss>=4.94654581764\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_e015877c-682a-4d49-b276-e880eb562c11-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.42595100402832, \"sum\": 8.42595100402832, \"min\": 8.42595100402832}}, \"EndTime\": 1609453373.256253, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453373.247488}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Epoch[7] Batch[0] avg_epoch_loss=4.762498\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=4.76249837875\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Epoch[7] Batch[5] avg_epoch_loss=4.668429\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.66842913628\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Epoch[7] Batch [5]#011Speed: 3480.68 samples/sec#011loss=4.668429\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 445.53208351135254, \"sum\": 445.53208351135254, \"min\": 445.53208351135254}}, \"EndTime\": 1609453373.701898, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453373.256311}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1406.98681787 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] #quality_metric: host=algo-1, epoch=7, train loss <loss>=4.60415730476\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:53 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_e6ae3695-c7d6-4026-9ccf-fd597e030945-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.442878723144531, \"sum\": 8.442878723144531, \"min\": 8.442878723144531}}, \"EndTime\": 1609453373.71079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453373.701965}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[8] Batch[0] avg_epoch_loss=4.357452\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=4.35745239258\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[8] Batch[5] avg_epoch_loss=4.321988\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=4.32198818525\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[8] Batch [5]#011Speed: 3715.32 samples/sec#011loss=4.321988\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[8] Batch[10] avg_epoch_loss=4.234549\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=4.12962112427\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[8] Batch [10]#011Speed: 3104.17 samples/sec#011loss=4.129621\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 502.89297103881836, \"sum\": 502.89297103881836, \"min\": 502.89297103881836}}, \"EndTime\": 1609453374.213826, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453373.710847}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1284.14627262 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #quality_metric: host=algo-1, epoch=8, train loss <loss>=4.23454861207\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_77c29a8f-c197-45f2-8ae0-878927637f11-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 15.22207260131836, \"sum\": 15.22207260131836, \"min\": 15.22207260131836}}, \"EndTime\": 1609453374.230204, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453374.213938}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[9] Batch[0] avg_epoch_loss=4.140063\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=4.14006280899\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[9] Batch[5] avg_epoch_loss=3.957926\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.95792555809\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[9] Batch [5]#011Speed: 2795.52 samples/sec#011loss=3.957926\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[9] Batch[10] avg_epoch_loss=3.888680\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=3.8055847168\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Epoch[9] Batch [10]#011Speed: 3016.94 samples/sec#011loss=3.805585\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 556.7681789398193, \"sum\": 556.7681789398193, \"min\": 556.7681789398193}}, \"EndTime\": 1609453374.78712, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453374.230276}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1151.03716218 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.88867972114\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:54 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_22765719-f6a8-4d4d-bbd0-6126968117bc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.888887405395508, \"sum\": 9.888887405395508, \"min\": 9.888887405395508}}, \"EndTime\": 1609453374.797521, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453374.787192}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Epoch[10] Batch[0] avg_epoch_loss=3.764250\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.76425004005\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Epoch[10] Batch[5] avg_epoch_loss=3.673964\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=3.67396398385\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Epoch[10] Batch [5]#011Speed: 2941.37 samples/sec#011loss=3.673964\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 595.8240032196045, \"sum\": 595.8240032196045, \"min\": 595.8240032196045}}, \"EndTime\": 1609453375.39355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453374.797642}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1021.85834735 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.64798996449\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_0c74d520-d85e-473d-899d-db44f8c9bfc1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.673999786376953, \"sum\": 10.673999786376953, \"min\": 10.673999786376953}}, \"EndTime\": 1609453375.405008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453375.393639}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Epoch[11] Batch[0] avg_epoch_loss=3.535332\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=3.53533172607\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Epoch[11] Batch[5] avg_epoch_loss=3.537189\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=3.53718940417\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Epoch[11] Batch [5]#011Speed: 3925.51 samples/sec#011loss=3.537189\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Epoch[11] Batch[10] avg_epoch_loss=3.523085\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=3.50616021156\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Epoch[11] Batch [10]#011Speed: 3736.92 samples/sec#011loss=3.506160\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 468.890905380249, \"sum\": 468.890905380249, \"min\": 468.890905380249}}, \"EndTime\": 1609453375.874067, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453375.405113}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1411.52862375 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] #quality_metric: host=algo-1, epoch=11, train loss <loss>=3.52308522571\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:55 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_ed72dc2c-1581-47ec-a81d-24ee37ea958b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.053852081298828, \"sum\": 7.053852081298828, \"min\": 7.053852081298828}}, \"EndTime\": 1609453375.881623, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453375.874136}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] Epoch[12] Batch[0] avg_epoch_loss=3.498123\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=3.49812269211\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] Epoch[12] Batch[5] avg_epoch_loss=3.482284\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=3.48228438695\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] Epoch[12] Batch [5]#011Speed: 3372.29 samples/sec#011loss=3.482284\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 442.2900676727295, \"sum\": 442.2900676727295, \"min\": 442.2900676727295}}, \"EndTime\": 1609453376.324023, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453375.881679}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1428.62924141 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #quality_metric: host=algo-1, epoch=12, train loss <loss>=3.44071395397\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_c5612d39-10d1-4987-a3eb-13e17a6111bf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.65283203125, \"sum\": 6.65283203125, \"min\": 6.65283203125}}, \"EndTime\": 1609453376.331233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453376.324085}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] Epoch[13] Batch[0] avg_epoch_loss=3.267109\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.26710915565\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] Epoch[13] Batch[5] avg_epoch_loss=3.337110\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=3.337109526\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] Epoch[13] Batch [5]#011Speed: 3601.94 samples/sec#011loss=3.337110\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 421.57506942749023, \"sum\": 421.57506942749023, \"min\": 421.57506942749023}}, \"EndTime\": 1609453376.752928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453376.331292}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1501.1451879 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] #quality_metric: host=algo-1, epoch=13, train loss <loss>=3.32052137852\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:56 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_f90ea82e-3ffe-4c86-8660-007be0fcb1b3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.705045700073242, \"sum\": 6.705045700073242, \"min\": 6.705045700073242}}, \"EndTime\": 1609453376.760194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453376.752996}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[14] Batch[0] avg_epoch_loss=3.365552\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.36555171013\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[14] Batch[5] avg_epoch_loss=3.310550\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=3.31055001418\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[14] Batch [5]#011Speed: 3277.14 samples/sec#011loss=3.310550\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[14] Batch[10] avg_epoch_loss=3.313662\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=3.31739611626\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[14] Batch [10]#011Speed: 3552.84 samples/sec#011loss=3.317396\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 458.1170082092285, \"sum\": 458.1170082092285, \"min\": 458.1170082092285}}, \"EndTime\": 1609453377.218429, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453376.760254}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1442.56327213 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.31366187876\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_7a68e9cd-0d32-493d-b923-2118d8f76f63-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.706974029541016, \"sum\": 9.706974029541016, \"min\": 9.706974029541016}}, \"EndTime\": 1609453377.228545, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453377.218493}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[15] Batch[0] avg_epoch_loss=3.231862\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.23186206818\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[15] Batch[5] avg_epoch_loss=3.259195\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=3.25919477145\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[15] Batch [5]#011Speed: 3226.23 samples/sec#011loss=3.259195\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[15] Batch[10] avg_epoch_loss=3.257355\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=3.25514817238\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Epoch[15] Batch [10]#011Speed: 3598.34 samples/sec#011loss=3.255148\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 498.8899230957031, \"sum\": 498.8899230957031, \"min\": 498.8899230957031}}, \"EndTime\": 1609453377.727549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453377.228604}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1372.7649685 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] #quality_metric: host=algo-1, epoch=15, train loss <loss>=3.25735540824\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:57 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_ad52b11a-7726-4902-aca1-71f0ec786eee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.644010543823242, \"sum\": 6.644010543823242, \"min\": 6.644010543823242}}, \"EndTime\": 1609453377.734691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453377.72762}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[16] Batch[0] avg_epoch_loss=3.201148\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=3.2011475563\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[16] Batch[5] avg_epoch_loss=3.218765\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.21876462301\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[16] Batch [5]#011Speed: 3993.46 samples/sec#011loss=3.218765\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[16] Batch[10] avg_epoch_loss=3.178782\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=3.13080306053\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[16] Batch [10]#011Speed: 3909.88 samples/sec#011loss=3.130803\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 430.264949798584, \"sum\": 430.264949798584, \"min\": 430.264949798584}}, \"EndTime\": 1609453378.165059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453377.734743}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1570.81160451 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.17878209461\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_15954a5c-24b1-4ac9-98d0-bdabec941cde-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.680011749267578, \"sum\": 6.680011749267578, \"min\": 6.680011749267578}}, \"EndTime\": 1609453378.172173, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453378.165115}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[17] Batch[0] avg_epoch_loss=3.207742\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=3.20774173737\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[17] Batch[5] avg_epoch_loss=3.171204\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=3.1712038517\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[17] Batch [5]#011Speed: 3908.11 samples/sec#011loss=3.171204\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[17] Batch[10] avg_epoch_loss=3.138901\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=3.10013680458\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[17] Batch [10]#011Speed: 3681.34 samples/sec#011loss=3.100137\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 455.8148384094238, \"sum\": 455.8148384094238, \"min\": 455.8148384094238}}, \"EndTime\": 1609453378.628105, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453378.172233}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1432.32205904 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=17, train loss <loss>=3.13890064846\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_29b0df19-3399-4e16-97f5-673d19c29a82-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.291006088256836, \"sum\": 8.291006088256836, \"min\": 8.291006088256836}}, \"EndTime\": 1609453378.636805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453378.628165}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[18] Batch[0] avg_epoch_loss=3.100250\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.10025048256\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[18] Batch[5] avg_epoch_loss=3.097095\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.09709485372\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:58 INFO 140699746432832] Epoch[18] Batch [5]#011Speed: 3818.88 samples/sec#011loss=3.097095\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[18] Batch[10] avg_epoch_loss=3.136660\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=3.18413724899\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[18] Batch [10]#011Speed: 3881.75 samples/sec#011loss=3.184137\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 438.6179447174072, \"sum\": 438.6179447174072, \"min\": 438.6179447174072}}, \"EndTime\": 1609453379.075538, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453378.636866}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1504.40011825 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #quality_metric: host=algo-1, epoch=18, train loss <loss>=3.13665957884\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_38e115df-2f9e-489e-b74d-0d0577b0723b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.61978530883789, \"sum\": 8.61978530883789, \"min\": 8.61978530883789}}, \"EndTime\": 1609453379.084561, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453379.075603}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[19] Batch[0] avg_epoch_loss=3.125291\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.1252913475\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[19] Batch[5] avg_epoch_loss=3.144249\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=3.14424860477\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[19] Batch [5]#011Speed: 3860.40 samples/sec#011loss=3.144249\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[19] Batch[10] avg_epoch_loss=3.090359\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=3.02569174767\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[19] Batch [10]#011Speed: 3897.07 samples/sec#011loss=3.025692\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 481.65011405944824, \"sum\": 481.65011405944824, \"min\": 481.65011405944824}}, \"EndTime\": 1609453379.566326, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453379.084622}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1378.32527179 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.09035912427\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_9b32cffe-8626-4773-b170-090c3841ed70-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.517980575561523, \"sum\": 8.517980575561523, \"min\": 8.517980575561523}}, \"EndTime\": 1609453379.575251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453379.566389}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[20] Batch[0] avg_epoch_loss=3.088495\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=3.08849453926\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[20] Batch[5] avg_epoch_loss=3.108424\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=3.10842351119\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:22:59 INFO 140699746432832] Epoch[20] Batch [5]#011Speed: 3777.32 samples/sec#011loss=3.108424\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Epoch[20] Batch[10] avg_epoch_loss=3.064165\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=3.0110555172\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Epoch[20] Batch [10]#011Speed: 3306.23 samples/sec#011loss=3.011056\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 459.7008228302002, \"sum\": 459.7008228302002, \"min\": 459.7008228302002}}, \"EndTime\": 1609453380.035071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453379.575313}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1505.06068969 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #quality_metric: host=algo-1, epoch=20, train loss <loss>=3.0641653321\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_c46ae823-644e-4231-888f-c1f9ab4aed49-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.17218017578125, \"sum\": 6.17218017578125, \"min\": 6.17218017578125}}, \"EndTime\": 1609453380.041584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453380.035121}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Epoch[21] Batch[0] avg_epoch_loss=3.096994\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=3.09699440002\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Epoch[21] Batch[5] avg_epoch_loss=3.080896\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=3.0808964173\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Epoch[21] Batch [5]#011Speed: 3509.20 samples/sec#011loss=3.080896\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 418.70713233947754, \"sum\": 418.70713233947754, \"min\": 418.70713233947754}}, \"EndTime\": 1609453380.460394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453380.041636}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1396.76187451 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #quality_metric: host=algo-1, epoch=21, train loss <loss>=3.05864555836\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_5734dbaa-38f8-44d2-b7ff-b10498ef3d18-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.93402099609375, \"sum\": 8.93402099609375, \"min\": 8.93402099609375}}, \"EndTime\": 1609453380.469806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453380.46048}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Epoch[22] Batch[0] avg_epoch_loss=3.022157\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=3.02215695381\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Epoch[22] Batch[5] avg_epoch_loss=3.061787\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=3.06178716818\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Epoch[22] Batch [5]#011Speed: 3989.03 samples/sec#011loss=3.061787\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 420.4239845275879, \"sum\": 420.4239845275879, \"min\": 420.4239845275879}}, \"EndTime\": 1609453380.890351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453380.46987}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1474.38239043 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.03740825653\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:00 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_299c9fae-cd15-47d5-8688-9b07f3f94da4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.597042083740234, \"sum\": 6.597042083740234, \"min\": 6.597042083740234}}, \"EndTime\": 1609453380.89746, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453380.890413}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] Epoch[23] Batch[0] avg_epoch_loss=3.145226\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.14522624016\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] Epoch[23] Batch[5] avg_epoch_loss=3.020820\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=3.02081978321\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] Epoch[23] Batch [5]#011Speed: 3235.39 samples/sec#011loss=3.020820\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 442.7480697631836, \"sum\": 442.7480697631836, \"min\": 442.7480697631836}}, \"EndTime\": 1609453381.340316, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453380.897509}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1411.29980915 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #quality_metric: host=algo-1, epoch=23, train loss <loss>=3.00255043507\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_dd64204a-c42d-4dae-9200-e950319e9210-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.05010986328125, \"sum\": 6.05010986328125, \"min\": 6.05010986328125}}, \"EndTime\": 1609453381.346954, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453381.340387}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] Epoch[24] Batch[0] avg_epoch_loss=3.093426\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=3.09342575073\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] Epoch[24] Batch[5] avg_epoch_loss=3.075635\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.07563495636\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] Epoch[24] Batch [5]#011Speed: 3479.20 samples/sec#011loss=3.075635\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 462.230920791626, \"sum\": 462.230920791626, \"min\": 462.230920791626}}, \"EndTime\": 1609453381.809298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453381.347013}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1345.35907584 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] #quality_metric: host=algo-1, epoch=24, train loss <loss>=3.09032492638\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:01 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[25] Batch[0] avg_epoch_loss=2.917177\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.91717720032\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[25] Batch[5] avg_epoch_loss=2.972369\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.97236939271\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[25] Batch [5]#011Speed: 3809.66 samples/sec#011loss=2.972369\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 446.21896743774414, \"sum\": 446.21896743774414, \"min\": 446.21896743774414}}, \"EndTime\": 1609453382.256042, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453381.809366}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1382.42267359 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #quality_metric: host=algo-1, epoch=25, train loss <loss>=3.00097367764\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_cc0af10e-44f4-42b6-a1fe-cb979795ac06-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.071111679077148, \"sum\": 9.071111679077148, \"min\": 9.071111679077148}}, \"EndTime\": 1609453382.265709, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453382.256105}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[26] Batch[0] avg_epoch_loss=2.922248\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.92224788666\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[26] Batch[5] avg_epoch_loss=2.987872\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.98787240187\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[26] Batch [5]#011Speed: 3765.90 samples/sec#011loss=2.987872\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[26] Batch[10] avg_epoch_loss=2.975022\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=2.95960168839\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[26] Batch [10]#011Speed: 3770.74 samples/sec#011loss=2.959602\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 455.69896697998047, \"sum\": 455.69896697998047, \"min\": 455.69896697998047}}, \"EndTime\": 1609453382.721526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453382.26577}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1478.75800185 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.97502207756\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_b11cda73-bfb3-408a-8ecd-58973321e831-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 5.928993225097656, \"sum\": 5.928993225097656, \"min\": 5.928993225097656}}, \"EndTime\": 1609453382.727913, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453382.721581}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] Epoch[27] Batch[0] avg_epoch_loss=3.062632\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:02 INFO 140699746432832] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.06263208389\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] Epoch[27] Batch[5] avg_epoch_loss=3.036715\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=3.03671499093\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] Epoch[27] Batch [5]#011Speed: 3728.47 samples/sec#011loss=3.036715\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 413.2370948791504, \"sum\": 413.2370948791504, \"min\": 413.2370948791504}}, \"EndTime\": 1609453383.141259, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453382.727968}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1509.67450505 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #quality_metric: host=algo-1, epoch=27, train loss <loss>=3.00157575607\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] Epoch[28] Batch[0] avg_epoch_loss=2.999822\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.99982237816\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] Epoch[28] Batch[5] avg_epoch_loss=2.970973\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.97097301483\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] Epoch[28] Batch [5]#011Speed: 3570.67 samples/sec#011loss=2.970973\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 422.98388481140137, \"sum\": 422.98388481140137, \"min\": 422.98388481140137}}, \"EndTime\": 1609453383.564768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453383.141323}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1484.3387351 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.99147355556\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] Epoch[29] Batch[0] avg_epoch_loss=3.014176\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.01417613029\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] Epoch[29] Batch[5] avg_epoch_loss=2.974031\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.97403132915\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] Epoch[29] Batch [5]#011Speed: 3993.81 samples/sec#011loss=2.974031\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 425.8849620819092, \"sum\": 425.8849620819092, \"min\": 425.8849620819092}}, \"EndTime\": 1609453383.991058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453383.564834}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1485.97412776 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.98853223324\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:03 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Epoch[30] Batch[0] avg_epoch_loss=2.870391\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.87039136887\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Epoch[30] Batch[5] avg_epoch_loss=2.979967\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.97996679942\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Epoch[30] Batch [5]#011Speed: 3278.38 samples/sec#011loss=2.979967\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Epoch[30] Batch[10] avg_epoch_loss=2.957600\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=2.93075909615\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Epoch[30] Batch [10]#011Speed: 2651.49 samples/sec#011loss=2.930759\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 494.27199363708496, \"sum\": 494.27199363708496, \"min\": 494.27199363708496}}, \"EndTime\": 1609453384.485739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453383.991126}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1324.92629463 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.95759966157\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_d657d89a-c84e-41a1-a014-73d60e993fc5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.069110870361328, \"sum\": 7.069110870361328, \"min\": 7.069110870361328}}, \"EndTime\": 1609453384.49331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453384.4858}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Epoch[31] Batch[0] avg_epoch_loss=2.842056\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.842056036\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Epoch[31] Batch[5] avg_epoch_loss=2.893275\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.89327474435\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Epoch[31] Batch [5]#011Speed: 3816.52 samples/sec#011loss=2.893275\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 415.9548282623291, \"sum\": 415.9548282623291, \"min\": 415.9548282623291}}, \"EndTime\": 1609453384.909382, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453384.49337}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1511.81168435 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.89830534458\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:04 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_ff3aef55-9b83-4a53-b0b7-84091b611e3d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.812904357910156, \"sum\": 8.812904357910156, \"min\": 8.812904357910156}}, \"EndTime\": 1609453384.918639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453384.909452}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[32] Batch[0] avg_epoch_loss=2.931328\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.93132758141\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[32] Batch[5] avg_epoch_loss=2.930800\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.9307996432\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[32] Batch [5]#011Speed: 3972.14 samples/sec#011loss=2.930800\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[32] Batch[10] avg_epoch_loss=2.929255\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=2.92740240097\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[32] Batch [10]#011Speed: 3627.50 samples/sec#011loss=2.927402\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 486.89794540405273, \"sum\": 486.89794540405273, \"min\": 486.89794540405273}}, \"EndTime\": 1609453385.405649, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453384.9187}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1420.97659235 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.92925544219\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[33] Batch[0] avg_epoch_loss=2.910722\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.91072225571\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[33] Batch[5] avg_epoch_loss=2.972974\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.97297441959\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[33] Batch [5]#011Speed: 4025.76 samples/sec#011loss=2.972974\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[33] Batch[10] avg_epoch_loss=2.916220\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.84811501503\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] Epoch[33] Batch [10]#011Speed: 3935.80 samples/sec#011loss=2.848115\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 443.6450004577637, \"sum\": 443.6450004577637, \"min\": 443.6450004577637}}, \"EndTime\": 1609453385.849648, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453385.405712}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1496.37854857 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.91622014479\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:05 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[34] Batch[0] avg_epoch_loss=3.154525\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=3.15452528\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[34] Batch[5] avg_epoch_loss=3.002939\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=3.00293934345\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[34] Batch [5]#011Speed: 3936.19 samples/sec#011loss=3.002939\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 435.6231689453125, \"sum\": 435.6231689453125, \"min\": 435.6231689453125}}, \"EndTime\": 1609453386.285624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453385.849709}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1464.23679352 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.96217498779\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[35] Batch[0] avg_epoch_loss=2.957593\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.95759296417\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[35] Batch[5] avg_epoch_loss=2.915846\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.91584595044\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[35] Batch [5]#011Speed: 3917.63 samples/sec#011loss=2.915846\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[35] Batch[10] avg_epoch_loss=2.869456\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=2.81378898621\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[35] Batch [10]#011Speed: 3870.62 samples/sec#011loss=2.813789\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 446.52605056762695, \"sum\": 446.52605056762695, \"min\": 446.52605056762695}}, \"EndTime\": 1609453386.732559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453386.285692}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1477.73639859 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.86945642125\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_9759a55b-80b2-4398-b839-ec0d515b870b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.546974182128906, \"sum\": 6.546974182128906, \"min\": 6.546974182128906}}, \"EndTime\": 1609453386.739577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453386.732629}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] Epoch[36] Batch[0] avg_epoch_loss=2.899461\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:06 INFO 140699746432832] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.89946126938\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[36] Batch[5] avg_epoch_loss=2.912437\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.91243731976\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[36] Batch [5]#011Speed: 4021.26 samples/sec#011loss=2.912437\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 410.9821319580078, \"sum\": 410.9821319580078, \"min\": 410.9821319580078}}, \"EndTime\": 1609453387.150664, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453386.739628}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1530.14551193 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.90090856552\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[37] Batch[0] avg_epoch_loss=2.892464\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.89246416092\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[37] Batch[5] avg_epoch_loss=2.920673\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.92067305247\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[37] Batch [5]#011Speed: 3272.05 samples/sec#011loss=2.920673\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[37] Batch[10] avg_epoch_loss=2.896023\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=2.86644206047\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[37] Batch [10]#011Speed: 3864.45 samples/sec#011loss=2.866442\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 455.4009437561035, \"sum\": 455.4009437561035, \"min\": 455.4009437561035}}, \"EndTime\": 1609453387.606538, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453387.150725}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1420.40780982 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.89602260156\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[38] Batch[0] avg_epoch_loss=2.896369\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.89636945724\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[38] Batch[5] avg_epoch_loss=2.893822\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.89382219315\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:07 INFO 140699746432832] Epoch[38] Batch [5]#011Speed: 3823.45 samples/sec#011loss=2.893822\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 416.31197929382324, \"sum\": 416.31197929382324, \"min\": 416.31197929382324}}, \"EndTime\": 1609453388.023302, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453387.606607}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1510.51935487 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.88270432949\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] Epoch[39] Batch[0] avg_epoch_loss=3.058974\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=3.05897402763\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] Epoch[39] Batch[5] avg_epoch_loss=2.904149\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.90414873759\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] Epoch[39] Batch [5]#011Speed: 3823.50 samples/sec#011loss=2.904149\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] Epoch[39] Batch[10] avg_epoch_loss=3.047248\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=3.21896634102\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] Epoch[39] Batch [10]#011Speed: 3717.56 samples/sec#011loss=3.218966\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 452.00300216674805, \"sum\": 452.00300216674805, \"min\": 452.00300216674805}}, \"EndTime\": 1609453388.475716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453388.02337}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1442.1624942 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.04724764824\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] Epoch[40] Batch[0] avg_epoch_loss=3.007658\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=3.00765776634\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] Epoch[40] Batch[5] avg_epoch_loss=2.942935\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.94293487072\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] Epoch[40] Batch [5]#011Speed: 3364.88 samples/sec#011loss=2.942935\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] processed a total of 583 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 422.558069229126, \"sum\": 422.558069229126, \"min\": 422.558069229126}}, \"EndTime\": 1609453388.898733, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453388.475778}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1379.32702845 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.89300246239\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:08 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] Epoch[41] Batch[0] avg_epoch_loss=2.973886\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.97388553619\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] Epoch[41] Batch[5] avg_epoch_loss=2.926403\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.92640284697\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] Epoch[41] Batch [5]#011Speed: 3631.74 samples/sec#011loss=2.926403\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 416.4259433746338, \"sum\": 416.4259433746338, \"min\": 416.4259433746338}}, \"EndTime\": 1609453389.315731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453388.898809}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1507.60084755 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.94267845154\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] Epoch[42] Batch[0] avg_epoch_loss=2.892926\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.89292550087\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] Epoch[42] Batch[5] avg_epoch_loss=2.912692\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.91269155343\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] Epoch[42] Batch [5]#011Speed: 3626.72 samples/sec#011loss=2.912692\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 448.261022567749, \"sum\": 448.261022567749, \"min\": 448.261022567749}}, \"EndTime\": 1609453389.764489, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453389.315795}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1362.69577614 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.90481636524\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:09 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[43] Batch[0] avg_epoch_loss=2.865609\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.86560916901\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[43] Batch[5] avg_epoch_loss=2.908970\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.90896964073\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[43] Batch [5]#011Speed: 4018.48 samples/sec#011loss=2.908970\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 410.214900970459, \"sum\": 410.214900970459, \"min\": 410.214900970459}}, \"EndTime\": 1609453390.175204, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453389.764555}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1491.5333895 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.89599785805\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[44] Batch[0] avg_epoch_loss=2.801858\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.8018579483\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[44] Batch[5] avg_epoch_loss=2.816357\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.81635737419\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[44] Batch [5]#011Speed: 3150.66 samples/sec#011loss=2.816357\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[44] Batch[10] avg_epoch_loss=2.832100\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=2.85099167824\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[44] Batch [10]#011Speed: 3484.36 samples/sec#011loss=2.850992\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 455.86204528808594, \"sum\": 455.86204528808594, \"min\": 455.86204528808594}}, \"EndTime\": 1609453390.631523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453390.175272}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1440.90973064 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.83210023967\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_bd248eb9-d4a9-4dc6-80f7-e2d7177ae77a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.558034896850586, \"sum\": 8.558034896850586, \"min\": 8.558034896850586}}, \"EndTime\": 1609453390.640497, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453390.63159}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[45] Batch[0] avg_epoch_loss=2.825273\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.82527256012\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[45] Batch[5] avg_epoch_loss=2.851179\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.85117916266\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:10 INFO 140699746432832] Epoch[45] Batch [5]#011Speed: 3600.60 samples/sec#011loss=2.851179\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 418.3619022369385, \"sum\": 418.3619022369385, \"min\": 418.3619022369385}}, \"EndTime\": 1609453391.05898, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453390.640563}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1412.34184955 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.84006726742\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] Epoch[46] Batch[0] avg_epoch_loss=2.749114\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.74911355972\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] Epoch[46] Batch[5] avg_epoch_loss=2.807608\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=2.8076078097\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] Epoch[46] Batch [5]#011Speed: 3999.90 samples/sec#011loss=2.807608\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 405.5459499359131, \"sum\": 405.5459499359131, \"min\": 405.5459499359131}}, \"EndTime\": 1609453391.464972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453391.059044}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1535.78363803 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #quality_metric: host=algo-1, epoch=46, train loss <loss>=2.79362194538\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_956e8e67-e309-4427-9f75-1d83189b4d44-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.864070892333984, \"sum\": 6.864070892333984, \"min\": 6.864070892333984}}, \"EndTime\": 1609453391.472353, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453391.465037}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] Epoch[47] Batch[0] avg_epoch_loss=2.934622\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.93462204933\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] Epoch[47] Batch[5] avg_epoch_loss=2.894045\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.89404527346\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] Epoch[47] Batch [5]#011Speed: 3631.00 samples/sec#011loss=2.894045\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 422.9168891906738, \"sum\": 422.9168891906738, \"min\": 422.9168891906738}}, \"EndTime\": 1609453391.895378, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453391.472404}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1475.1179341 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.90606348515\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:11 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[48] Batch[0] avg_epoch_loss=2.920338\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=2.92033791542\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[48] Batch[5] avg_epoch_loss=2.854688\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.85468792915\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[48] Batch [5]#011Speed: 3545.87 samples/sec#011loss=2.854688\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[48] Batch[10] avg_epoch_loss=2.898381\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=2.95081214905\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[48] Batch [10]#011Speed: 3890.20 samples/sec#011loss=2.950812\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 435.8041286468506, \"sum\": 435.8041286468506, \"min\": 435.8041286468506}}, \"EndTime\": 1609453392.331645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453391.895444}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1495.77318085 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.89838075638\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[49] Batch[0] avg_epoch_loss=2.932128\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.93212795258\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[49] Batch[5] avg_epoch_loss=2.844993\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.84499283632\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[49] Batch [5]#011Speed: 3626.27 samples/sec#011loss=2.844993\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[49] Batch[10] avg_epoch_loss=2.847592\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=2.85071129799\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] Epoch[49] Batch [10]#011Speed: 3716.56 samples/sec#011loss=2.850711\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 440.85001945495605, \"sum\": 440.85001945495605, \"min\": 440.85001945495605}}, \"EndTime\": 1609453392.772918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453392.331704}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1471.84555833 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.84759213708\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:12 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[50] Batch[0] avg_epoch_loss=2.831394\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.83139371872\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[50] Batch[5] avg_epoch_loss=2.922103\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=2.92210324605\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[50] Batch [5]#011Speed: 3927.43 samples/sec#011loss=2.922103\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[50] Batch[10] avg_epoch_loss=2.892250\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=2.85642590523\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[50] Batch [10]#011Speed: 3841.72 samples/sec#011loss=2.856426\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 431.9109916687012, \"sum\": 431.9109916687012, \"min\": 431.9109916687012}}, \"EndTime\": 1609453393.205186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453392.77298}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1537.01950153 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=50, train loss <loss>=2.89224990931\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[51] Batch[0] avg_epoch_loss=2.907433\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.90743303299\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[51] Batch[5] avg_epoch_loss=2.888433\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.88843313853\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[51] Batch [5]#011Speed: 3450.74 samples/sec#011loss=2.888433\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[51] Batch[10] avg_epoch_loss=2.816497\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=2.73017473221\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[51] Batch [10]#011Speed: 3815.48 samples/sec#011loss=2.730175\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 463.5028839111328, \"sum\": 463.5028839111328, \"min\": 463.5028839111328}}, \"EndTime\": 1609453393.669052, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453393.20525}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1447.37760219 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.81649749929\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] Epoch[52] Batch[0] avg_epoch_loss=2.691308\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:13 INFO 140699746432832] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=2.69130825996\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Epoch[52] Batch[5] avg_epoch_loss=2.818139\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.8181385994\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Epoch[52] Batch [5]#011Speed: 3710.91 samples/sec#011loss=2.818139\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Epoch[52] Batch[10] avg_epoch_loss=2.771278\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=2.71504592896\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Epoch[52] Batch [10]#011Speed: 3918.73 samples/sec#011loss=2.715046\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 467.526912689209, \"sum\": 467.526912689209, \"min\": 467.526912689209}}, \"EndTime\": 1609453394.136942, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453393.669117}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1385.7465192 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.77127829465\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_75fd8124-96fb-4b8d-bdf7-4466344da553-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.81814956665039, \"sum\": 8.81814956665039, \"min\": 8.81814956665039}}, \"EndTime\": 1609453394.146157, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453394.137005}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Epoch[53] Batch[0] avg_epoch_loss=2.772005\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.77200484276\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Epoch[53] Batch[5] avg_epoch_loss=2.743856\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.74385639032\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Epoch[53] Batch [5]#011Speed: 3956.38 samples/sec#011loss=2.743856\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 459.8040580749512, \"sum\": 459.8040580749512, \"min\": 459.8040580749512}}, \"EndTime\": 1609453394.606085, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453394.146222}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1356.79025898 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.78025090694\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] Epoch[54] Batch[0] avg_epoch_loss=2.879933\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:14 INFO 140699746432832] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.87993311882\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[54] Batch[5] avg_epoch_loss=2.769557\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.76955707868\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[54] Batch [5]#011Speed: 3528.86 samples/sec#011loss=2.769557\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 483.9041233062744, \"sum\": 483.9041233062744, \"min\": 483.9041233062744}}, \"EndTime\": 1609453395.090731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453394.606155}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1256.19843126 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.79396202564\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[55] Batch[0] avg_epoch_loss=2.738543\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.7385430336\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[55] Batch[5] avg_epoch_loss=2.858268\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.85826770465\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[55] Batch [5]#011Speed: 3643.32 samples/sec#011loss=2.858268\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[55] Batch[10] avg_epoch_loss=2.845676\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=2.83056702614\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[55] Batch [10]#011Speed: 3367.64 samples/sec#011loss=2.830567\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 446.72489166259766, \"sum\": 446.72489166259766, \"min\": 446.72489166259766}}, \"EndTime\": 1609453395.537972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453395.090793}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1436.82396294 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.84567648714\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[56] Batch[0] avg_epoch_loss=2.773242\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.77324151993\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[56] Batch[5] avg_epoch_loss=2.877908\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.87790795167\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[56] Batch [5]#011Speed: 3759.22 samples/sec#011loss=2.877908\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[56] Batch[10] avg_epoch_loss=2.841876\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=2.7986374855\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] Epoch[56] Batch [10]#011Speed: 3796.01 samples/sec#011loss=2.798637\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 459.9909782409668, \"sum\": 459.9909782409668, \"min\": 459.9909782409668}}, \"EndTime\": 1609453395.998327, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453395.538036}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1451.90890341 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.8418759216\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:15 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Epoch[57] Batch[0] avg_epoch_loss=2.692895\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=2.69289517403\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Epoch[57] Batch[5] avg_epoch_loss=2.758509\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=2.75850868225\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Epoch[57] Batch [5]#011Speed: 3972.01 samples/sec#011loss=2.758509\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Epoch[57] Batch[10] avg_epoch_loss=2.751132\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=2.74227905273\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Epoch[57] Batch [10]#011Speed: 3903.26 samples/sec#011loss=2.742279\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 463.47999572753906, \"sum\": 463.47999572753906, \"min\": 463.47999572753906}}, \"EndTime\": 1609453396.462177, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453395.99839}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1408.61815268 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #quality_metric: host=algo-1, epoch=57, train loss <loss>=2.75113157793\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_b70e738f-e6bc-4c77-8997-8043439e97ed-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.4228515625, \"sum\": 8.4228515625, \"min\": 8.4228515625}}, \"EndTime\": 1609453396.471033, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453396.462241}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Epoch[58] Batch[0] avg_epoch_loss=2.793807\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.79380702972\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Epoch[58] Batch[5] avg_epoch_loss=2.761448\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=2.76144798597\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] Epoch[58] Batch [5]#011Speed: 3713.89 samples/sec#011loss=2.761448\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 432.6009750366211, \"sum\": 432.6009750366211, \"min\": 432.6009750366211}}, \"EndTime\": 1609453396.903747, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453396.471091}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1467.5499573 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] #quality_metric: host=algo-1, epoch=58, train loss <loss>=2.75830359459\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:16 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[59] Batch[0] avg_epoch_loss=2.777795\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=2.77779531479\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[59] Batch[5] avg_epoch_loss=2.774359\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=2.77435855071\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[59] Batch [5]#011Speed: 3719.33 samples/sec#011loss=2.774359\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[59] Batch[10] avg_epoch_loss=2.763365\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=2.75017175674\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[59] Batch [10]#011Speed: 3617.46 samples/sec#011loss=2.750172\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 457.4921131134033, \"sum\": 457.4921131134033, \"min\": 457.4921131134033}}, \"EndTime\": 1609453397.361647, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453396.903809}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1501.32097467 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #quality_metric: host=algo-1, epoch=59, train loss <loss>=2.76336455345\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[60] Batch[0] avg_epoch_loss=2.848948\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=2.84894800186\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[60] Batch[5] avg_epoch_loss=2.784863\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.78486291567\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[60] Batch [5]#011Speed: 3465.98 samples/sec#011loss=2.784863\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[60] Batch[10] avg_epoch_loss=2.719184\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=2.6403696537\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Epoch[60] Batch [10]#011Speed: 3913.60 samples/sec#011loss=2.640370\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 444.9629783630371, \"sum\": 444.9629783630371, \"min\": 444.9629783630371}}, \"EndTime\": 1609453397.807062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453397.36172}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1440.2744877 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.71918416023\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:17 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_2032a3eb-d74a-4ae9-a34d-5fd28785aa2c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.543968200683594, \"sum\": 8.543968200683594, \"min\": 8.543968200683594}}, \"EndTime\": 1609453397.816002, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453397.807122}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Epoch[61] Batch[0] avg_epoch_loss=2.706650\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.70664954185\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Epoch[61] Batch[5] avg_epoch_loss=2.799481\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.79948135217\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Epoch[61] Batch [5]#011Speed: 3297.88 samples/sec#011loss=2.799481\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Epoch[61] Batch[10] avg_epoch_loss=2.711353\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=2.60560007095\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Epoch[61] Batch [10]#011Speed: 3233.14 samples/sec#011loss=2.605600\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 500.31208992004395, \"sum\": 500.31208992004395, \"min\": 500.31208992004395}}, \"EndTime\": 1609453398.31643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453397.816062}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1290.92584534 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.71135349707\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_3c0ea96e-c9aa-4357-b4e9-6b69a2b4e948-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.387233734130859, \"sum\": 6.387233734130859, \"min\": 6.387233734130859}}, \"EndTime\": 1609453398.323315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453398.316504}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Epoch[62] Batch[0] avg_epoch_loss=2.628370\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.62837028503\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Epoch[62] Batch[5] avg_epoch_loss=2.712246\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.71224617958\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] Epoch[62] Batch [5]#011Speed: 3784.75 samples/sec#011loss=2.712246\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 411.24987602233887, \"sum\": 411.24987602233887, \"min\": 411.24987602233887}}, \"EndTime\": 1609453398.734681, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453398.323377}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1429.45037947 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.77320785522\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:18 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] Epoch[63] Batch[0] avg_epoch_loss=2.786480\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.78647994995\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] Epoch[63] Batch[5] avg_epoch_loss=2.797507\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.79750712713\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] Epoch[63] Batch [5]#011Speed: 3844.58 samples/sec#011loss=2.797507\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 449.82314109802246, \"sum\": 449.82314109802246, \"min\": 449.82314109802246}}, \"EndTime\": 1609453399.184911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453398.734746}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1420.25870267 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.80787329674\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] Epoch[64] Batch[0] avg_epoch_loss=2.802941\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.80294060707\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] Epoch[64] Batch[5] avg_epoch_loss=2.749657\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.74965711435\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] Epoch[64] Batch [5]#011Speed: 3839.14 samples/sec#011loss=2.749657\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 438.48395347595215, \"sum\": 438.48395347595215, \"min\": 438.48395347595215}}, \"EndTime\": 1609453399.623835, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453399.184976}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1370.17871735 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.70764997005\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_5d2a3717-a82e-4a52-bd57-d014525ce18a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.2770843505859375, \"sum\": 6.2770843505859375, \"min\": 6.2770843505859375}}, \"EndTime\": 1609453399.630929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453399.623909}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] Epoch[65] Batch[0] avg_epoch_loss=2.816603\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:19 INFO 140699746432832] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=2.81660342216\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[65] Batch[5] avg_epoch_loss=2.760031\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.76003114382\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[65] Batch [5]#011Speed: 3423.82 samples/sec#011loss=2.760031\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[65] Batch[10] avg_epoch_loss=2.815594\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=2.88226852417\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[65] Batch [10]#011Speed: 3766.84 samples/sec#011loss=2.882269\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 473.0710983276367, \"sum\": 473.0710983276367, \"min\": 473.0710983276367}}, \"EndTime\": 1609453400.104105, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453399.630979}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1426.57003541 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.81559358944\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[66] Batch[0] avg_epoch_loss=2.625992\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=2.62599182129\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[66] Batch[5] avg_epoch_loss=2.739301\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=2.73930136363\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[66] Batch [5]#011Speed: 4031.35 samples/sec#011loss=2.739301\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 415.4222011566162, \"sum\": 415.4222011566162, \"min\": 415.4222011566162}}, \"EndTime\": 1609453400.519884, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453400.104169}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1513.79558786 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=66, train loss <loss>=2.78099007607\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[67] Batch[0] avg_epoch_loss=2.884659\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=2.88465881348\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[67] Batch[5] avg_epoch_loss=2.692737\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=2.69273670514\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[67] Batch [5]#011Speed: 3991.07 samples/sec#011loss=2.692737\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[67] Batch[10] avg_epoch_loss=2.694128\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=2.69579687119\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Epoch[67] Batch [10]#011Speed: 3644.15 samples/sec#011loss=2.695797\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 459.43593978881836, \"sum\": 459.43593978881836, \"min\": 459.43593978881836}}, \"EndTime\": 1609453400.979806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453400.519945}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1468.88566408 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] #quality_metric: host=algo-1, epoch=67, train loss <loss>=2.69412768971\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:20 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_64c174a9-ac2c-4f2f-a39f-df33338da2d3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.4228515625, \"sum\": 8.4228515625, \"min\": 8.4228515625}}, \"EndTime\": 1609453400.988634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453400.97987}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] Epoch[68] Batch[0] avg_epoch_loss=2.656074\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=2.65607380867\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] Epoch[68] Batch[5] avg_epoch_loss=2.768511\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=2.76851077875\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] Epoch[68] Batch [5]#011Speed: 4006.15 samples/sec#011loss=2.768511\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 460.22987365722656, \"sum\": 460.22987365722656, \"min\": 460.22987365722656}}, \"EndTime\": 1609453401.448972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453400.988691}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1353.40914209 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #quality_metric: host=algo-1, epoch=68, train loss <loss>=2.74010956287\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] Epoch[69] Batch[0] avg_epoch_loss=2.639007\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.6390068531\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] Epoch[69] Batch[5] avg_epoch_loss=2.757692\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.7576918602\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] Epoch[69] Batch [5]#011Speed: 3892.19 samples/sec#011loss=2.757692\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 415.6980514526367, \"sum\": 415.6980514526367, \"min\": 415.6980514526367}}, \"EndTime\": 1609453401.865084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453401.449035}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1519.99865134 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.73939926624\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:21 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[70] Batch[0] avg_epoch_loss=2.742783\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=2.74278259277\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[70] Batch[5] avg_epoch_loss=2.714244\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=2.71424396833\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[70] Batch [5]#011Speed: 3626.14 samples/sec#011loss=2.714244\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 411.1630916595459, \"sum\": 411.1630916595459, \"min\": 411.1630916595459}}, \"EndTime\": 1609453402.276672, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453401.865142}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1483.26850342 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #quality_metric: host=algo-1, epoch=70, train loss <loss>=2.71913785934\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[71] Batch[0] avg_epoch_loss=2.589916\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=2.58991599083\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[71] Batch[5] avg_epoch_loss=2.702505\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=2.70250499249\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[71] Batch [5]#011Speed: 3985.55 samples/sec#011loss=2.702505\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[71] Batch[10] avg_epoch_loss=2.686740\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=2.66782159805\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[71] Batch [10]#011Speed: 3653.22 samples/sec#011loss=2.667822\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 435.1468086242676, \"sum\": 435.1468086242676, \"min\": 435.1468086242676}}, \"EndTime\": 1609453402.712236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453402.276734}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1534.83154074 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #quality_metric: host=algo-1, epoch=71, train loss <loss>=2.6867398132\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_a6d00c93-6146-4791-994b-4eda60d6d941-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.6928863525390625, \"sum\": 6.6928863525390625, \"min\": 6.6928863525390625}}, \"EndTime\": 1609453402.719369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453402.712288}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] Epoch[72] Batch[0] avg_epoch_loss=2.812148\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:22 INFO 140699746432832] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=2.81214785576\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[72] Batch[5] avg_epoch_loss=2.803670\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=2.80366988977\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[72] Batch [5]#011Speed: 3347.70 samples/sec#011loss=2.803670\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[72] Batch[10] avg_epoch_loss=2.704808\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=2.58617320061\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[72] Batch [10]#011Speed: 3337.22 samples/sec#011loss=2.586173\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 463.5319709777832, \"sum\": 463.5319709777832, \"min\": 463.5319709777832}}, \"EndTime\": 1609453403.183004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453402.719418}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1389.03878988 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #quality_metric: host=algo-1, epoch=72, train loss <loss>=2.70480775833\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[73] Batch[0] avg_epoch_loss=2.637422\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.63742232323\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[73] Batch[5] avg_epoch_loss=2.688424\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=2.68842438857\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[73] Batch [5]#011Speed: 3811.61 samples/sec#011loss=2.688424\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 421.6461181640625, \"sum\": 421.6461181640625, \"min\": 421.6461181640625}}, \"EndTime\": 1609453403.605011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453403.183071}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1505.642118 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #quality_metric: host=algo-1, epoch=73, train loss <loss>=2.70458528996\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[74] Batch[0] avg_epoch_loss=2.871983\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=2.87198257446\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[74] Batch[5] avg_epoch_loss=2.745450\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=2.74545021852\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:23 INFO 140699746432832] Epoch[74] Batch [5]#011Speed: 3845.78 samples/sec#011loss=2.745450\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Epoch[74] Batch[10] avg_epoch_loss=2.711443\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=2.67063369751\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Epoch[74] Batch [10]#011Speed: 3433.04 samples/sec#011loss=2.670634\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 452.8939723968506, \"sum\": 452.8939723968506, \"min\": 452.8939723968506}}, \"EndTime\": 1609453404.05841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453403.605081}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1472.44238102 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #quality_metric: host=algo-1, epoch=74, train loss <loss>=2.71144270897\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Epoch[75] Batch[0] avg_epoch_loss=2.613724\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=2.61372351646\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Epoch[75] Batch[5] avg_epoch_loss=2.737399\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=2.73739918073\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Epoch[75] Batch [5]#011Speed: 3831.51 samples/sec#011loss=2.737399\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 442.43812561035156, \"sum\": 442.43812561035156, \"min\": 442.43812561035156}}, \"EndTime\": 1609453404.501216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453404.058472}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1389.72082714 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #quality_metric: host=algo-1, epoch=75, train loss <loss>=2.73305289745\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Epoch[76] Batch[0] avg_epoch_loss=2.776628\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=2.77662849426\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Epoch[76] Batch[5] avg_epoch_loss=2.710330\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=2.71032992999\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Epoch[76] Batch [5]#011Speed: 3968.50 samples/sec#011loss=2.710330\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 470.9739685058594, \"sum\": 470.9739685058594, \"min\": 470.9739685058594}}, \"EndTime\": 1609453404.972595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453404.501279}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1335.25788448 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] #quality_metric: host=algo-1, epoch=76, train loss <loss>=2.68314306736\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:24 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_bf10fe60-0384-401d-b9f9-e77ecb3ec473-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.11314582824707, \"sum\": 8.11314582824707, \"min\": 8.11314582824707}}, \"EndTime\": 1609453404.981135, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453404.97266}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] Epoch[77] Batch[0] avg_epoch_loss=2.781610\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.78160953522\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] Epoch[77] Batch[5] avg_epoch_loss=2.741202\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.74120247364\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] Epoch[77] Batch [5]#011Speed: 3894.99 samples/sec#011loss=2.741202\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 417.1450138092041, \"sum\": 417.1450138092041, \"min\": 417.1450138092041}}, \"EndTime\": 1609453405.398391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453404.981193}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1466.79629306 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #quality_metric: host=algo-1, epoch=77, train loss <loss>=2.72233495712\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] Epoch[78] Batch[0] avg_epoch_loss=2.751385\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=2.75138473511\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] Epoch[78] Batch[5] avg_epoch_loss=2.687234\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=2.68723372618\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] Epoch[78] Batch [5]#011Speed: 3474.48 samples/sec#011loss=2.687234\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 420.48096656799316, \"sum\": 420.48096656799316, \"min\": 420.48096656799316}}, \"EndTime\": 1609453405.81934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453405.398451}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1462.29074535 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] #quality_metric: host=algo-1, epoch=78, train loss <loss>=2.68962013721\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:25 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[79] Batch[0] avg_epoch_loss=2.719824\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.7198240757\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[79] Batch[5] avg_epoch_loss=2.674760\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=2.67475966612\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[79] Batch [5]#011Speed: 3364.59 samples/sec#011loss=2.674760\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 427.8759956359863, \"sum\": 427.8759956359863, \"min\": 427.8759956359863}}, \"EndTime\": 1609453406.247649, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453405.819401}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1399.64117229 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #quality_metric: host=algo-1, epoch=79, train loss <loss>=2.74484524727\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[80] Batch[0] avg_epoch_loss=2.586147\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=2.58614706993\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[80] Batch[5] avg_epoch_loss=2.620440\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=2.62044008573\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[80] Batch [5]#011Speed: 3973.71 samples/sec#011loss=2.620440\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[80] Batch[10] avg_epoch_loss=2.670124\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=2.72974462509\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[80] Batch [10]#011Speed: 3931.16 samples/sec#011loss=2.729745\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 431.20718002319336, \"sum\": 431.20718002319336, \"min\": 431.20718002319336}}, \"EndTime\": 1609453406.679333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453406.247712}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1569.61782399 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #quality_metric: host=algo-1, epoch=80, train loss <loss>=2.67012396726\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_07064603-4e39-4a12-8068-e2385a4b408a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.352041244506836, \"sum\": 8.352041244506836, \"min\": 8.352041244506836}}, \"EndTime\": 1609453406.68814, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453406.679409}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] Epoch[81] Batch[0] avg_epoch_loss=2.901241\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:26 INFO 140699746432832] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=2.90124106407\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[81] Batch[5] avg_epoch_loss=2.773084\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=2.77308388551\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[81] Batch [5]#011Speed: 3798.44 samples/sec#011loss=2.773084\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[81] Batch[10] avg_epoch_loss=2.738599\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=2.69721674919\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[81] Batch [10]#011Speed: 3431.85 samples/sec#011loss=2.697217\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 468.6131477355957, \"sum\": 468.6131477355957, \"min\": 468.6131477355957}}, \"EndTime\": 1609453407.156869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453406.688199}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1440.1413713 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #quality_metric: host=algo-1, epoch=81, train loss <loss>=2.73859882355\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[82] Batch[0] avg_epoch_loss=2.796337\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=2.79633688927\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[82] Batch[5] avg_epoch_loss=2.748791\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=2.74879141649\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[82] Batch [5]#011Speed: 3638.10 samples/sec#011loss=2.748791\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 418.2469844818115, \"sum\": 418.2469844818115, \"min\": 418.2469844818115}}, \"EndTime\": 1609453407.575534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453407.156926}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1479.65696556 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #quality_metric: host=algo-1, epoch=82, train loss <loss>=2.70435905457\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[83] Batch[0] avg_epoch_loss=2.700348\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=2.70034837723\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[83] Batch[5] avg_epoch_loss=2.671875\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=2.67187519868\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:27 INFO 140699746432832] Epoch[83] Batch [5]#011Speed: 3839.64 samples/sec#011loss=2.671875\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[83] Batch[10] avg_epoch_loss=2.668165\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=2.66371302605\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[83] Batch [10]#011Speed: 3704.82 samples/sec#011loss=2.663713\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 443.48692893981934, \"sum\": 443.48692893981934, \"min\": 443.48692893981934}}, \"EndTime\": 1609453408.019446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453407.575597}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1512.65833831 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=83, train loss <loss>=2.66816512021\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_f9d4692c-90f3-4152-8e16-1e86ea78dbe7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.577968597412109, \"sum\": 6.577968597412109, \"min\": 6.577968597412109}}, \"EndTime\": 1609453408.026562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453408.019516}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[84] Batch[0] avg_epoch_loss=2.781691\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=2.78169131279\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[84] Batch[5] avg_epoch_loss=2.664792\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=2.66479214032\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[84] Batch [5]#011Speed: 3969.39 samples/sec#011loss=2.664792\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[84] Batch[10] avg_epoch_loss=2.632033\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=2.59272203445\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[84] Batch [10]#011Speed: 3589.82 samples/sec#011loss=2.592722\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 435.20307540893555, \"sum\": 435.20307540893555, \"min\": 435.20307540893555}}, \"EndTime\": 1609453408.461881, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453408.026623}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1557.55379752 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=84, train loss <loss>=2.63203300129\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_e66dc540-512d-430c-b20f-8ff4f9f329fb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.155895233154297, \"sum\": 7.155895233154297, \"min\": 7.155895233154297}}, \"EndTime\": 1609453408.46952, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453408.461946}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[85] Batch[0] avg_epoch_loss=2.820321\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=2.82032108307\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[85] Batch[5] avg_epoch_loss=2.652825\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=2.65282523632\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] Epoch[85] Batch [5]#011Speed: 3612.47 samples/sec#011loss=2.652825\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 415.50207138061523, \"sum\": 415.50207138061523, \"min\": 415.50207138061523}}, \"EndTime\": 1609453408.885138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453408.46958}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1515.88130008 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] #quality_metric: host=algo-1, epoch=85, train loss <loss>=2.67548298836\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:28 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] Epoch[86] Batch[0] avg_epoch_loss=2.708280\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=2.70828008652\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] Epoch[86] Batch[5] avg_epoch_loss=2.732060\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=2.73205991586\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] Epoch[86] Batch [5]#011Speed: 3798.40 samples/sec#011loss=2.732060\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 483.60514640808105, \"sum\": 483.60514640808105, \"min\": 483.60514640808105}}, \"EndTime\": 1609453409.369158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453408.885204}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1292.12630251 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #quality_metric: host=algo-1, epoch=86, train loss <loss>=2.65620789528\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] Epoch[87] Batch[0] avg_epoch_loss=2.629943\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=2.62994265556\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] Epoch[87] Batch[5] avg_epoch_loss=2.667861\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=2.6678609848\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] Epoch[87] Batch [5]#011Speed: 3834.18 samples/sec#011loss=2.667861\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 462.5670909881592, \"sum\": 462.5670909881592, \"min\": 462.5670909881592}}, \"EndTime\": 1609453409.832125, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453409.369223}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1374.55310619 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] #quality_metric: host=algo-1, epoch=87, train loss <loss>=2.67289557457\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:29 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] Epoch[88] Batch[0] avg_epoch_loss=2.685371\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=2.68537068367\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] Epoch[88] Batch[5] avg_epoch_loss=2.662783\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=2.66278278828\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] Epoch[88] Batch [5]#011Speed: 3971.01 samples/sec#011loss=2.662783\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 430.59611320495605, \"sum\": 430.59611320495605, \"min\": 430.59611320495605}}, \"EndTime\": 1609453410.263461, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453409.832216}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1472.02940528 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #quality_metric: host=algo-1, epoch=88, train loss <loss>=2.67206993103\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] Epoch[89] Batch[0] avg_epoch_loss=2.748503\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=2.74850344658\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] Epoch[89] Batch[5] avg_epoch_loss=2.674381\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=2.67438121637\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] Epoch[89] Batch [5]#011Speed: 3855.37 samples/sec#011loss=2.674381\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 407.4370861053467, \"sum\": 407.4370861053467, \"min\": 407.4370861053467}}, \"EndTime\": 1609453410.671413, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453410.263524}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1454.98222354 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #quality_metric: host=algo-1, epoch=89, train loss <loss>=2.64828681946\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] Epoch[90] Batch[0] avg_epoch_loss=2.843129\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:30 INFO 140699746432832] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=2.84312915802\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[90] Batch[5] avg_epoch_loss=2.714708\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=2.71470808983\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[90] Batch [5]#011Speed: 3643.93 samples/sec#011loss=2.714708\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 422.3630428314209, \"sum\": 422.3630428314209, \"min\": 422.3630428314209}}, \"EndTime\": 1609453411.094269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453410.671479}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1415.51516821 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=90, train loss <loss>=2.63950567245\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[91] Batch[0] avg_epoch_loss=2.434178\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=2.43417835236\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[91] Batch[5] avg_epoch_loss=2.572607\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=2.5726073583\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[91] Batch [5]#011Speed: 3824.10 samples/sec#011loss=2.572607\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[91] Batch[10] avg_epoch_loss=2.668586\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=2.78376040459\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[91] Batch [10]#011Speed: 3969.28 samples/sec#011loss=2.783760\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 426.16915702819824, \"sum\": 426.16915702819824, \"min\": 426.16915702819824}}, \"EndTime\": 1609453411.52088, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453411.094336}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1522.52706186 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=91, train loss <loss>=2.6685860157\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[92] Batch[0] avg_epoch_loss=2.580512\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=2.5805118084\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[92] Batch[5] avg_epoch_loss=2.624725\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=2.6247253418\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[92] Batch [5]#011Speed: 3540.57 samples/sec#011loss=2.624725\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[92] Batch[10] avg_epoch_loss=2.632644\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=2.64214725494\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] Epoch[92] Batch [10]#011Speed: 3901.72 samples/sec#011loss=2.642147\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 453.1998634338379, \"sum\": 453.1998634338379, \"min\": 453.1998634338379}}, \"EndTime\": 1609453411.974434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453411.520944}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1449.39033785 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] #quality_metric: host=algo-1, epoch=92, train loss <loss>=2.63264439323\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:31 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[93] Batch[0] avg_epoch_loss=2.649616\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=2.64961600304\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[93] Batch[5] avg_epoch_loss=2.712669\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=2.71266861757\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[93] Batch [5]#011Speed: 3825.69 samples/sec#011loss=2.712669\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[93] Batch[10] avg_epoch_loss=2.640086\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=2.55298728943\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[93] Batch [10]#011Speed: 3826.88 samples/sec#011loss=2.552987\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 459.7909450531006, \"sum\": 459.7909450531006, \"min\": 459.7909450531006}}, \"EndTime\": 1609453412.434592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453411.974498}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1469.93612606 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #quality_metric: host=algo-1, epoch=93, train loss <loss>=2.64008619569\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[94] Batch[0] avg_epoch_loss=2.619635\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=2.61963534355\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[94] Batch[5] avg_epoch_loss=2.668744\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=2.66874353091\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[94] Batch [5]#011Speed: 4001.96 samples/sec#011loss=2.668744\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[94] Batch[10] avg_epoch_loss=2.623937\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=2.57016963959\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Epoch[94] Batch [10]#011Speed: 3773.63 samples/sec#011loss=2.570170\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 451.434850692749, \"sum\": 451.434850692749, \"min\": 451.434850692749}}, \"EndTime\": 1609453412.886391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453412.434655}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1450.66180027 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] #quality_metric: host=algo-1, epoch=94, train loss <loss>=2.62393721667\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:32 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_6f9f2dc4-5f7e-4d63-b1af-85ca92a596ed-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.696939468383789, \"sum\": 6.696939468383789, \"min\": 6.696939468383789}}, \"EndTime\": 1609453412.893531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453412.886445}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] Epoch[95] Batch[0] avg_epoch_loss=2.739177\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=2.7391769886\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] Epoch[95] Batch[5] avg_epoch_loss=2.664243\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=2.66424282392\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] Epoch[95] Batch [5]#011Speed: 3534.19 samples/sec#011loss=2.664243\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 419.4478988647461, \"sum\": 419.4478988647461, \"min\": 419.4478988647461}}, \"EndTime\": 1609453413.313096, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453412.893589}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1506.38635956 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #quality_metric: host=algo-1, epoch=95, train loss <loss>=2.6305729866\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] Epoch[96] Batch[0] avg_epoch_loss=2.583638\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=2.58363771439\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] Epoch[96] Batch[5] avg_epoch_loss=2.684020\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=2.68401972453\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] Epoch[96] Batch [5]#011Speed: 3774.74 samples/sec#011loss=2.684020\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 463.11306953430176, \"sum\": 463.11306953430176, \"min\": 463.11306953430176}}, \"EndTime\": 1609453413.776614, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453413.313163}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1334.16503607 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] #quality_metric: host=algo-1, epoch=96, train loss <loss>=2.6721421957\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:33 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Epoch[97] Batch[0] avg_epoch_loss=2.643584\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=2.64358353615\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Epoch[97] Batch[5] avg_epoch_loss=2.642034\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=2.64203413328\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Epoch[97] Batch [5]#011Speed: 3493.15 samples/sec#011loss=2.642034\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 482.6090335845947, \"sum\": 482.6090335845947, \"min\": 482.6090335845947}}, \"EndTime\": 1609453414.25962, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453413.77668}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1301.00711695 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #quality_metric: host=algo-1, epoch=97, train loss <loss>=2.60911931992\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_ac898502-4c9f-4c71-b9ea-07763d9b3b0b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.683826446533203, \"sum\": 6.683826446533203, \"min\": 6.683826446533203}}, \"EndTime\": 1609453414.266865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453414.259684}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Epoch[98] Batch[0] avg_epoch_loss=2.751495\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=2.75149536133\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Epoch[98] Batch[5] avg_epoch_loss=2.632990\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=2.63299024105\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Epoch[98] Batch [5]#011Speed: 3626.76 samples/sec#011loss=2.632990\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Epoch[98] Batch[10] avg_epoch_loss=2.611312\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=2.58529839516\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] Epoch[98] Batch [10]#011Speed: 3897.26 samples/sec#011loss=2.585298\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 436.816930770874, \"sum\": 436.816930770874, \"min\": 436.816930770874}}, \"EndTime\": 1609453414.703794, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453414.266921}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1494.58401768 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] #quality_metric: host=algo-1, epoch=98, train loss <loss>=2.61131212928\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:34 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[99] Batch[0] avg_epoch_loss=2.740064\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=2.7400636673\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[99] Batch[5] avg_epoch_loss=2.717431\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=2.71743134658\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[99] Batch [5]#011Speed: 3889.30 samples/sec#011loss=2.717431\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 444.4570541381836, \"sum\": 444.4570541381836, \"min\": 444.4570541381836}}, \"EndTime\": 1609453415.14864, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453414.703857}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1414.91459779 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #quality_metric: host=algo-1, epoch=99, train loss <loss>=2.68497524261\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[100] Batch[0] avg_epoch_loss=2.682529\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=2.68252873421\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[100] Batch[5] avg_epoch_loss=2.634728\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=2.6347275575\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[100] Batch [5]#011Speed: 3939.82 samples/sec#011loss=2.634728\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[100] Batch[10] avg_epoch_loss=2.613883\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=2.58886880875\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[100] Batch [10]#011Speed: 3828.85 samples/sec#011loss=2.588869\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 478.56879234313965, \"sum\": 478.56879234313965, \"min\": 478.56879234313965}}, \"EndTime\": 1609453415.627623, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453415.148703}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1391.38189195 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #quality_metric: host=algo-1, epoch=100, train loss <loss>=2.6138826717\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] Epoch[101] Batch[0] avg_epoch_loss=2.600847\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:35 INFO 140699746432832] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=2.60084652901\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[101] Batch[5] avg_epoch_loss=2.632199\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=2.63219936689\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[101] Batch [5]#011Speed: 3876.34 samples/sec#011loss=2.632199\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[101] Batch[10] avg_epoch_loss=2.578288\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=2.51359534264\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[101] Batch [10]#011Speed: 3919.32 samples/sec#011loss=2.513595\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 473.4079837799072, \"sum\": 473.4079837799072, \"min\": 473.4079837799072}}, \"EndTime\": 1609453416.10139, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453415.627684}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1357.97524168 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=101, train loss <loss>=2.57828844677\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/state_00fc50de-b355-4595-b5ac-a3c78589b552-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 6.567955017089844, \"sum\": 6.567955017089844, \"min\": 6.567955017089844}}, \"EndTime\": 1609453416.10844, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453416.101448}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[102] Batch[0] avg_epoch_loss=2.748031\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=2.74803113937\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[102] Batch[5] avg_epoch_loss=2.678179\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=2.67817914486\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[102] Batch [5]#011Speed: 3904.20 samples/sec#011loss=2.678179\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[102] Batch[10] avg_epoch_loss=2.615222\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=2.53967380524\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[102] Batch [10]#011Speed: 3846.07 samples/sec#011loss=2.539674\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 436.0520839691162, \"sum\": 436.0520839691162, \"min\": 436.0520839691162}}, \"EndTime\": 1609453416.544619, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453416.10851}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1513.24473677 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=102, train loss <loss>=2.6152221723\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[103] Batch[0] avg_epoch_loss=2.577532\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=2.57753181458\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[103] Batch[5] avg_epoch_loss=2.607297\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=2.60729678472\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:36 INFO 140699746432832] Epoch[103] Batch [5]#011Speed: 3600.00 samples/sec#011loss=2.607297\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 458.7068557739258, \"sum\": 458.7068557739258, \"min\": 458.7068557739258}}, \"EndTime\": 1609453417.003688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453416.544684}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1377.48554752 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #quality_metric: host=algo-1, epoch=103, train loss <loss>=2.63540117741\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] Epoch[104] Batch[0] avg_epoch_loss=2.818897\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=2.81889748573\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] Epoch[104] Batch[5] avg_epoch_loss=2.663786\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=2.66378593445\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] Epoch[104] Batch [5]#011Speed: 3642.32 samples/sec#011loss=2.663786\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 424.06201362609863, \"sum\": 424.06201362609863, \"min\": 424.06201362609863}}, \"EndTime\": 1609453417.42816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453417.003757}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1506.5103527 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #quality_metric: host=algo-1, epoch=104, train loss <loss>=2.65173089504\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] Epoch[105] Batch[0] avg_epoch_loss=2.544326\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=2.54432559013\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] Epoch[105] Batch[5] avg_epoch_loss=2.593685\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=2.59368459384\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] Epoch[105] Batch [5]#011Speed: 3704.13 samples/sec#011loss=2.593685\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] Epoch[105] Batch[10] avg_epoch_loss=2.625334\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=2.66331415176\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] Epoch[105] Batch [10]#011Speed: 3912.60 samples/sec#011loss=2.663314\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 443.389892578125, \"sum\": 443.389892578125, \"min\": 443.389892578125}}, \"EndTime\": 1609453417.871974, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453417.428225}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1571.67143511 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] #quality_metric: host=algo-1, epoch=105, train loss <loss>=2.62533439289\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:37 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[106] Batch[0] avg_epoch_loss=2.635607\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=2.63560652733\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[106] Batch[5] avg_epoch_loss=2.606123\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=2.60612293084\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[106] Batch [5]#011Speed: 3923.77 samples/sec#011loss=2.606123\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[106] Batch[10] avg_epoch_loss=2.661346\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=2.7276134491\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[106] Batch [10]#011Speed: 3940.79 samples/sec#011loss=2.727613\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 458.70208740234375, \"sum\": 458.70208740234375, \"min\": 458.70208740234375}}, \"EndTime\": 1609453418.331032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453417.872033}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1412.39823273 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #quality_metric: host=algo-1, epoch=106, train loss <loss>=2.66134589369\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[107] Batch[0] avg_epoch_loss=2.738930\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=2.73892974854\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[107] Batch[5] avg_epoch_loss=2.640038\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=2.6400377353\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[107] Batch [5]#011Speed: 3982.28 samples/sec#011loss=2.640038\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[107] Batch[10] avg_epoch_loss=2.619558\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=2.59498276711\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] Epoch[107] Batch [10]#011Speed: 3956.82 samples/sec#011loss=2.594983\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 432.3689937591553, \"sum\": 432.3689937591553, \"min\": 432.3689937591553}}, \"EndTime\": 1609453418.763808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453418.331094}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1509.97348858 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] #quality_metric: host=algo-1, epoch=107, train loss <loss>=2.6195582043\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:38 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[108] Batch[0] avg_epoch_loss=2.798835\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=2.79883503914\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[108] Batch[5] avg_epoch_loss=2.665219\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=2.66521851222\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[108] Batch [5]#011Speed: 3891.90 samples/sec#011loss=2.665219\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 410.72893142700195, \"sum\": 410.72893142700195, \"min\": 410.72893142700195}}, \"EndTime\": 1609453419.174965, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453418.763866}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1504.28223922 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=108, train loss <loss>=2.66230885983\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[109] Batch[0] avg_epoch_loss=2.747515\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=2.74751496315\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[109] Batch[5] avg_epoch_loss=2.634061\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=2.63406097889\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[109] Batch [5]#011Speed: 3829.41 samples/sec#011loss=2.634061\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[109] Batch[10] avg_epoch_loss=2.673262\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=2.72030329704\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[109] Batch [10]#011Speed: 3774.21 samples/sec#011loss=2.720303\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 436.4781379699707, \"sum\": 436.4781379699707, \"min\": 436.4781379699707}}, \"EndTime\": 1609453419.61194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453419.175029}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1536.97752186 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=109, train loss <loss>=2.6732620326\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[110] Batch[0] avg_epoch_loss=2.550700\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=2.55069994926\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[110] Batch[5] avg_epoch_loss=2.654275\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=2.65427450339\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:39 INFO 140699746432832] Epoch[110] Batch [5]#011Speed: 3333.30 samples/sec#011loss=2.654275\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Epoch[110] Batch[10] avg_epoch_loss=2.591702\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=2.516614151\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Epoch[110] Batch [10]#011Speed: 3190.40 samples/sec#011loss=2.516614\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 463.93895149230957, \"sum\": 463.93895149230957, \"min\": 463.93895149230957}}, \"EndTime\": 1609453420.076298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453419.612}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1409.37344757 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, epoch=110, train loss <loss>=2.59170161594\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Epoch[111] Batch[0] avg_epoch_loss=2.475378\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=2.47537827492\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Epoch[111] Batch[5] avg_epoch_loss=2.654557\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=2.65455655257\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Epoch[111] Batch [5]#011Speed: 3998.60 samples/sec#011loss=2.654557\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Epoch[111] Batch[10] avg_epoch_loss=2.604126\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=2.54360971451\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Epoch[111] Batch [10]#011Speed: 3706.11 samples/sec#011loss=2.543610\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 445.65701484680176, \"sum\": 445.65701484680176, \"min\": 445.65701484680176}}, \"EndTime\": 1609453420.522359, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453420.076365}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #throughput_metric: host=algo-1, train throughput=1500.84128517 records/second\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, epoch=111, train loss <loss>=2.60412617163\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Loading parameters from best epoch (101)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 3.916025161743164, \"sum\": 3.916025161743164, \"min\": 3.916025161743164}}, \"EndTime\": 1609453420.526673, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453420.522422}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] stopping training now\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Final loss: 2.57828844677 (occurred at epoch 101)\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, train final_loss <loss>=2.57828844677\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 WARNING 140699746432832] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 39.00313377380371, \"sum\": 39.00313377380371, \"min\": 39.00313377380371}}, \"EndTime\": 1609453420.566263, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453420.52673}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 54.1079044342041, \"sum\": 54.1079044342041, \"min\": 54.1079044342041}}, \"EndTime\": 1609453420.581336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453420.566328}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 2.768993377685547, \"sum\": 2.768993377685547, \"min\": 2.768993377685547}}, \"EndTime\": 1609453420.584203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453420.581393}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.032901763916015625, \"sum\": 0.032901763916015625, \"min\": 0.032901763916015625}}, \"EndTime\": 1609453420.584899, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453420.584243}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 284.73591804504395, \"sum\": 284.73591804504395, \"min\": 284.73591804504395}}, \"EndTime\": 1609453420.869604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453420.584946}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, RMSE): 2.21383251\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, mean_absolute_QuantileLoss): 7.436507839626736\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, mean_wQuantileLoss): 0.011648792792308151\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.1]): 0.006263636616561647\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.2]): 0.007594418855098334\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.3]): 0.009213529397536061\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.4]): 0.014152275587457864\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.5]): 0.015300091372083243\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.6]): 0.016004302719717735\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.7]): 0.01585864231046603\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.8]): 0.012776187347093737\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #test_score (algo-1, wQuantileLoss[0.9]): 0.007676050924758715\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.0116487927923\u001b[0m\n",
      "\u001b[34m[12/31/2020 22:23:40 INFO 140699746432832] #quality_metric: host=algo-1, test RMSE <loss>=2.21383251\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 50960.85596084595, \"sum\": 50960.85596084595, \"min\": 50960.85596084595}, \"setuptime\": {\"count\": 1, \"max\": 9.08207893371582, \"sum\": 9.08207893371582, \"min\": 9.08207893371582}}, \"EndTime\": 1609453420.876621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609453420.869675}\n",
      "\u001b[0m\n",
      "\n",
      "2020-12-31 22:24:09 Uploading - Uploading generated training model\n",
      "2020-12-31 22:24:09 Completed - Training job completed\n",
      "Training seconds: 100\n",
      "Billable seconds: 100\n"
     ]
    }
   ],
   "source": [
    "# This step takes around 35 minutes to train the model with m4.xlarge instance\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job name: deepar-AAPL-with-dynamic-feat-2020-12-31-22-20-03-227\n"
     ]
    }
   ],
   "source": [
    "print ('job name: {0}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Endpoint with name \"deepar-AAPL-with-dynamic-feat-2020-12-31-22-20-03-227\" already exists; please pick a different name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-8284c25f1ccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     role=role)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_job\u001b[0;34m(self, job_name, initial_instance_count, instance_type, image_uri, name, role, wait, model_environment_vars, vpc_config_override, accelerator_type, data_capture_config)\u001b[0m\n\u001b[1;32m   3368\u001b[0m             \u001b[0mmodel_vpc_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvpc_config_override\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m             \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3371\u001b[0m         )\n\u001b[1;32m   3372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_model_data\u001b[0;34m(self, model_s3_location, image_uri, initial_instance_count, instance_type, name, role, wait, model_environment_vars, model_vpc_config, accelerator_type, data_capture_config)\u001b[0m\n\u001b[1;32m   3428\u001b[0m         ):\n\u001b[1;32m   3429\u001b[0m             raise ValueError(\n\u001b[0;32m-> 3430\u001b[0;31m                 \u001b[0;34m'Endpoint with name \"{}\" already exists; please pick a different name.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3431\u001b[0m             )\n\u001b[1;32m   3432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Endpoint with name \"deepar-AAPL-with-dynamic-feat-2020-12-31-22-20-03-227\" already exists; please pick a different name."
     ]
    }
   ],
   "source": [
    "# Create an endpoint for real-time predictions\n",
    "# SDK 2. parameter name for container: image_uri\n",
    "\n",
    "endpoint_name = sess.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    image_uri=container,\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: deepar-AAPL-with-dynamic-feat-2020-12-31-22-20-03-227\n"
     ]
    }
   ],
   "source": [
    "print ('endpoint name: {0}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the next lab, we will use the above endpoint for inference\n",
    "# We will delete the endpoint in the next lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
