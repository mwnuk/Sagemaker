{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR Model - Predict AAPL stock price  \n",
    "\n",
    "Note: This dataset is not a true timeseries as there a lot of gaps\n",
    "\n",
    "We have data only for first 20 days of each month and model needs to predict the rentals for \n",
    "the remaining days of the month. The dataset consists of two years data. DeepAR will shine with true multiple-timeseries dataset like the electricity example given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# This code is derived from AWS SageMaker Samples:\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a good base job name when building different models\n",
    "# It will help in identifying trained models and endpoints\n",
    "with_categories = False\n",
    "if with_categories:\n",
    "    base_job_name = 'APAL-with-categories'\n",
    "else:\n",
    "    base_job_name = 'AAPL-no-categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your bucket name\n",
    "bucket = 'mw-ml-sagemaker'\n",
    "prefix = 'deepar/AAPL'\n",
    "\n",
    "# This structure allows multiple training and test files for model development and testing\n",
    "if with_categories:\n",
    "    s3_data_path = \"{}/{}/data_with_categories\".format(bucket, prefix)\n",
    "else:\n",
    "    s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "    \n",
    "\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mw-ml-sagemaker/deepar/AAPL/data', 'mw-ml-sagemaker/deepar/AAPL/output')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path,s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name is referred as key name in S3\n",
    "# Files stored in S3 are automatically replicated across\n",
    "# three different availability zones in the region where the bucket was created.\n",
    "# http://boto3.readthedocs.io/en/latest/guide/s3.html\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload one or more training files and test files to S3\n",
    "if with_categories:\n",
    "    write_to_s3('train_with_categories.json',bucket,'deepar/AAPL/data_with_categories/train/train_with_categories.json')\n",
    "    write_to_s3('test_with_categories.json',bucket,'deepar/AAPL/data_with_categories/test/test_with_categories.json')\n",
    "else:\n",
    "    write_to_s3('AAPLtrain.json',bucket,'deepar/AAPL/data/AAPLtrain/train.json')\n",
    "    write_to_s3('AAPLtest.json',bucket,'deepar/AAPL/data/AAPLtest/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a session with AWS\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::480536818350:role/service-role/AmazonSageMaker-ExecutionRole-20201001T191047\n"
     ]
    }
   ],
   "source": [
    "# This role contains the permissions needed to train, deploy models\n",
    "# SageMaker Service is trusted to assume this role\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DeepAR Container 522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "# https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html#sagemaker.image_uris.retrieve\n",
    "\n",
    "# SDK 2 uses image_uris.retrieve the container image location\n",
    "\n",
    "# Use DeepAR Container\n",
    "container = sagemaker.image_uris.retrieve(\"forecasting-deepar\",sess.boto_region_name)\n",
    "\n",
    "print (f'Using DeepAR Container {container}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='D' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 5\n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training job\n",
    "# Specify type and number of instances to use\n",
    "#   Reference: http://sagemaker.readthedocs.io/en/latest/estimators.html\n",
    "# SDK 2.x version does not require train prefix for instance count and type\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=\"s3://\" + s3_output_path,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name=base_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D', 15, 5)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_freq': 'D',\n",
       " 'epochs': '400',\n",
       " 'early_stopping_patience': '10',\n",
       " 'mini_batch_size': '64',\n",
       " 'learning_rate': '5E-4',\n",
       " 'context_length': '15',\n",
       " 'prediction_length': '5',\n",
       " 'cardinality': ''}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are simply referring to train path and test path\n",
    "# You can have multiple files in each path\n",
    "# SageMaker will use all the files\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/AAPLtrain/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/AAPLtest/\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://mw-ml-sagemaker/deepar/AAPL/data/AAPLtrain/',\n",
       " 'test': 's3://mw-ml-sagemaker/deepar/AAPL/data/AAPLtest/'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-30 23:42:14 Starting - Starting the training job...\n",
      "2020-12-30 23:42:38 Starting - Launching requested ML instancesProfilerReport-1609371734: InProgress\n",
      ".........\n",
      "2020-12-30 23:43:59 Starting - Preparing the instances for training...\n",
      "2020-12-30 23:44:39 Downloading - Downloading input data...\n",
      "2020-12-30 23:45:05 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:06 INFO 140347361175360] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:06 INFO 140347361175360] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'5', u'epochs': u'400', u'time_freq': u'D', u'context_length': u'15', u'mini_batch_size': u'64', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:06 INFO 140347361175360] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'5', u'time_freq': u'D', u'context_length': u'15', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:06 INFO 140347361175360] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Training set statistics:\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Real time series\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] number of time series: 1\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] number of observations: 351\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] mean target length: 351\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] min/mean/max target: 53.0979995728/92.1961137821/134.179992676\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] mean abs(target): 92.1961137821\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] contains missing values: no\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Small number of time series. Doing 640 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Test set statistics:\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Real time series\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] number of time series: 1\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] number of observations: 366\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] mean target length: 366\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] min/mean/max target: 53.0979995728/93.5431181694/134.179992676\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] mean abs(target): 93.5431181694\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] contains missing values: no\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] nvidia-smi took: 0.025162935257 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 43.892860412597656, \"sum\": 43.892860412597656, \"min\": 43.892860412597656}}, \"EndTime\": 1609371907.199615, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371907.154758}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 133.53204727172852, \"sum\": 133.53204727172852, \"min\": 133.53204727172852}}, \"EndTime\": 1609371907.288417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371907.199697}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Epoch[0] Batch[0] avg_epoch_loss=6.527019\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=6.52701854706\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Epoch[0] Batch[5] avg_epoch_loss=6.112191\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=6.11219104131\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Epoch[0] Batch [5]#011Speed: 1947.03 samples/sec#011loss=6.112191\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Epoch[0] Batch[10] avg_epoch_loss=5.837197\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=5.50720310211\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Epoch[0] Batch [10]#011Speed: 1939.13 samples/sec#011loss=5.507203\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 605.9191226959229, \"sum\": 605.9191226959229, \"min\": 605.9191226959229}}, \"EndTime\": 1609371907.89448, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371907.288475}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1065.96601445 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] #quality_metric: host=algo-1, epoch=0, train loss <loss>=5.83719652349\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:07 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_1a2a9c06-920f-4846-94e9-298cd831dc29-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.396003723144531, \"sum\": 10.396003723144531, \"min\": 10.396003723144531}}, \"EndTime\": 1609371907.905353, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371907.894552}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[1] Batch[0] avg_epoch_loss=5.224842\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=5.22484207153\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[1] Batch[5] avg_epoch_loss=5.136697\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=5.13669721286\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[1] Batch [5]#011Speed: 1982.26 samples/sec#011loss=5.136697\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[1] Batch[10] avg_epoch_loss=5.116466\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=5.0921880722\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[1] Batch [10]#011Speed: 1942.70 samples/sec#011loss=5.092188\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.6200046539307, \"sum\": 528.6200046539307, \"min\": 528.6200046539307}}, \"EndTime\": 1609371908.434121, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371907.905444}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1218.04722239 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #quality_metric: host=algo-1, epoch=1, train loss <loss>=5.11646578529\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_6b597094-dd1c-4692-93f7-825067556e58-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.603023529052734, \"sum\": 9.603023529052734, \"min\": 9.603023529052734}}, \"EndTime\": 1609371908.444207, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371908.434185}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[2] Batch[0] avg_epoch_loss=4.919020\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=4.9190196991\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[2] Batch[5] avg_epoch_loss=4.892420\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=4.89241981506\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[2] Batch [5]#011Speed: 1904.48 samples/sec#011loss=4.892420\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[2] Batch[10] avg_epoch_loss=4.765589\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=4.61339225769\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Epoch[2] Batch [10]#011Speed: 1922.57 samples/sec#011loss=4.613392\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.7089042663574, \"sum\": 535.7089042663574, \"min\": 535.7089042663574}}, \"EndTime\": 1609371908.98003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371908.444266}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1216.83789993 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] #quality_metric: host=algo-1, epoch=2, train loss <loss>=4.76558910717\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:08 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_c4b5ca0e-cc1d-49cb-af74-611a1e53baa8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.814977645874023, \"sum\": 9.814977645874023, \"min\": 9.814977645874023}}, \"EndTime\": 1609371908.990402, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371908.980104}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Epoch[3] Batch[0] avg_epoch_loss=4.525996\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=4.52599573135\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Epoch[3] Batch[5] avg_epoch_loss=4.407018\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=4.40701754888\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Epoch[3] Batch [5]#011Speed: 2003.63 samples/sec#011loss=4.407018\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Epoch[3] Batch[10] avg_epoch_loss=4.253076\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=4.06834545135\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Epoch[3] Batch [10]#011Speed: 2005.60 samples/sec#011loss=4.068345\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 512.1431350708008, \"sum\": 512.1431350708008, \"min\": 512.1431350708008}}, \"EndTime\": 1609371909.502663, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371908.990467}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1255.30034444 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] #quality_metric: host=algo-1, epoch=3, train loss <loss>=4.25307568637\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_356c6571-cc61-4c30-b476-fa8748f5b18c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 12.687921524047852, \"sum\": 12.687921524047852, \"min\": 12.687921524047852}}, \"EndTime\": 1609371909.515732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371909.50272}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Epoch[4] Batch[0] avg_epoch_loss=4.093089\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.0930891037\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Epoch[4] Batch[5] avg_epoch_loss=3.959653\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.95965286096\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:09 INFO 140347361175360] Epoch[4] Batch [5]#011Speed: 1526.41 samples/sec#011loss=3.959653\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] Epoch[4] Batch[10] avg_epoch_loss=3.851405\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=3.72150707245\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] Epoch[4] Batch [10]#011Speed: 1425.72 samples/sec#011loss=3.721507\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 651.047945022583, \"sum\": 651.047945022583, \"min\": 651.047945022583}}, \"EndTime\": 1609371910.166957, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371909.515789}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1016.55043836 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #quality_metric: host=algo-1, epoch=4, train loss <loss>=3.85140477527\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_6cc3f0f9-d007-4c56-a35f-33549187643e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 15.269041061401367, \"sum\": 15.269041061401367, \"min\": 15.269041061401367}}, \"EndTime\": 1609371910.182746, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371910.167031}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] Epoch[5] Batch[0] avg_epoch_loss=3.597337\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.5973367691\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] Epoch[5] Batch[5] avg_epoch_loss=3.523134\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=3.52313363552\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] Epoch[5] Batch [5]#011Speed: 1409.00 samples/sec#011loss=3.523134\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 741.1010265350342, \"sum\": 741.1010265350342, \"min\": 741.1010265350342}}, \"EndTime\": 1609371910.923968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371910.182811}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=796.011884983 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] #quality_metric: host=algo-1, epoch=5, train loss <loss>=3.42796983719\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:10 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_a22e8b6e-780f-4f9a-ac37-fa04077b2af1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 12.035131454467773, \"sum\": 12.035131454467773, \"min\": 12.035131454467773}}, \"EndTime\": 1609371910.936639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371910.924031}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Epoch[6] Batch[0] avg_epoch_loss=3.273637\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.27363657951\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Epoch[6] Batch[5] avg_epoch_loss=3.254448\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.25444845359\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Epoch[6] Batch [5]#011Speed: 1876.64 samples/sec#011loss=3.254448\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Epoch[6] Batch[10] avg_epoch_loss=3.187561\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=3.10729680061\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Epoch[6] Batch [10]#011Speed: 1756.28 samples/sec#011loss=3.107297\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 628.5829544067383, \"sum\": 628.5829544067383, \"min\": 628.5829544067383}}, \"EndTime\": 1609371911.565346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371910.936704}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1052.99476049 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.1875613386\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_c6cfbf37-c92f-405c-997a-16d6618693ef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.866952896118164, \"sum\": 9.866952896118164, \"min\": 9.866952896118164}}, \"EndTime\": 1609371911.575669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371911.565413}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Epoch[7] Batch[0] avg_epoch_loss=3.080835\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.08083534241\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Epoch[7] Batch[5] avg_epoch_loss=3.099741\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=3.09974050522\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:11 INFO 140347361175360] Epoch[7] Batch [5]#011Speed: 1928.58 samples/sec#011loss=3.099741\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Epoch[7] Batch[10] avg_epoch_loss=3.058791\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=3.00965223312\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Epoch[7] Batch [10]#011Speed: 1695.61 samples/sec#011loss=3.009652\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 548.0339527130127, \"sum\": 548.0339527130127, \"min\": 548.0339527130127}}, \"EndTime\": 1609371912.123815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371911.575721}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1173.059914 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.05879129063\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_42e35911-3dfc-4adb-9aa5-4f7437d8c848-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.626150131225586, \"sum\": 9.626150131225586, \"min\": 9.626150131225586}}, \"EndTime\": 1609371912.13397, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371912.123886}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Epoch[8] Batch[0] avg_epoch_loss=2.925088\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=2.92508840561\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Epoch[8] Batch[5] avg_epoch_loss=2.943290\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=2.94328975677\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Epoch[8] Batch [5]#011Speed: 1909.47 samples/sec#011loss=2.943290\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Epoch[8] Batch[10] avg_epoch_loss=2.979483\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=3.02291498184\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Epoch[8] Batch [10]#011Speed: 1921.00 samples/sec#011loss=3.022915\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 546.8611717224121, \"sum\": 546.8611717224121, \"min\": 546.8611717224121}}, \"EndTime\": 1609371912.680957, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371912.134038}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1208.50657949 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #quality_metric: host=algo-1, epoch=8, train loss <loss>=2.9794830409\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_35cb07d4-5a98-46bd-a71e-91e09a402e81-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.855913162231445, \"sum\": 10.855913162231445, \"min\": 10.855913162231445}}, \"EndTime\": 1609371912.692258, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371912.681021}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] Epoch[9] Batch[0] avg_epoch_loss=2.936314\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:12 INFO 140347361175360] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=2.93631434441\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] Epoch[9] Batch[5] avg_epoch_loss=2.943561\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=2.94356071949\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] Epoch[9] Batch [5]#011Speed: 1714.18 samples/sec#011loss=2.943561\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] Epoch[9] Batch[10] avg_epoch_loss=3.020339\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=3.11247191429\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] Epoch[9] Batch [10]#011Speed: 1941.97 samples/sec#011loss=3.112472\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.6640720367432, \"sum\": 545.6640720367432, \"min\": 545.6640720367432}}, \"EndTime\": 1609371913.238044, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371912.692322}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1200.16981591 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.02033853531\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] Epoch[10] Batch[0] avg_epoch_loss=3.508947\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.5089468956\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] Epoch[10] Batch[5] avg_epoch_loss=3.120716\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=3.1207160155\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] Epoch[10] Batch [5]#011Speed: 1988.89 samples/sec#011loss=3.120716\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 504.4870376586914, \"sum\": 504.4870376586914, \"min\": 504.4870376586914}}, \"EndTime\": 1609371913.742893, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371913.238106}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1264.41071002 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.0672747612\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] Epoch[11] Batch[0] avg_epoch_loss=2.876887\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:13 INFO 140347361175360] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=2.87688660622\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[11] Batch[5] avg_epoch_loss=2.982652\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.98265214761\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[11] Batch [5]#011Speed: 1689.29 samples/sec#011loss=2.982652\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[11] Batch[10] avg_epoch_loss=2.919681\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=2.84411659241\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[11] Batch [10]#011Speed: 1952.30 samples/sec#011loss=2.844117\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] processed a total of 717 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 576.5719413757324, \"sum\": 576.5719413757324, \"min\": 576.5719413757324}}, \"EndTime\": 1609371914.319961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371913.742956}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1243.36252592 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #quality_metric: host=algo-1, epoch=11, train loss <loss>=2.93697583675\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_2b6c0c76-c3dc-4cca-86c3-c7aaa908d3d7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.28807258605957, \"sum\": 9.28807258605957, \"min\": 9.28807258605957}}, \"EndTime\": 1609371914.32971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371914.320018}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[12] Batch[0] avg_epoch_loss=2.805576\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.80557608604\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[12] Batch[5] avg_epoch_loss=2.869229\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.86922883987\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[12] Batch [5]#011Speed: 1967.23 samples/sec#011loss=2.869229\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[12] Batch[10] avg_epoch_loss=2.809867\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=2.73863196373\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Epoch[12] Batch [10]#011Speed: 1950.16 samples/sec#011loss=2.738632\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 561.5952014923096, \"sum\": 561.5952014923096, \"min\": 561.5952014923096}}, \"EndTime\": 1609371914.89142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371914.329771}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1189.2891088 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.80986662344\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:14 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_b71accd1-965a-4c98-8f94-b32b4fc095c8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 11.174917221069336, \"sum\": 11.174917221069336, \"min\": 11.174917221069336}}, \"EndTime\": 1609371914.90305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371914.891475}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[13] Batch[0] avg_epoch_loss=2.892842\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.89284157753\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[13] Batch[5] avg_epoch_loss=2.843297\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.84329672654\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[13] Batch [5]#011Speed: 1962.01 samples/sec#011loss=2.843297\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[13] Batch[10] avg_epoch_loss=2.842864\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=2.84234442711\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[13] Batch [10]#011Speed: 1984.17 samples/sec#011loss=2.842344\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 518.2609558105469, \"sum\": 518.2609558105469, \"min\": 518.2609558105469}}, \"EndTime\": 1609371915.421421, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371914.903109}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1279.06353913 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.84286386316\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[14] Batch[0] avg_epoch_loss=2.839750\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.83974957466\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[14] Batch[5] avg_epoch_loss=2.779145\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.77914464474\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[14] Batch [5]#011Speed: 1967.19 samples/sec#011loss=2.779145\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[14] Batch[10] avg_epoch_loss=2.805150\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=2.83635616302\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Epoch[14] Batch [10]#011Speed: 1901.71 samples/sec#011loss=2.836356\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.1620140075684, \"sum\": 541.1620140075684, \"min\": 541.1620140075684}}, \"EndTime\": 1609371915.962997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371915.42148}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1263.70629231 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.80514988032\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:15 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_0fbf1b4a-81a0-45e5-a83c-1d8f0a684021-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.809017181396484, \"sum\": 9.809017181396484, \"min\": 9.809017181396484}}, \"EndTime\": 1609371915.973334, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371915.963067}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Epoch[15] Batch[0] avg_epoch_loss=2.718934\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.71893358231\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Epoch[15] Batch[5] avg_epoch_loss=2.778707\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.77870738506\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Epoch[15] Batch [5]#011Speed: 1815.32 samples/sec#011loss=2.778707\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Epoch[15] Batch[10] avg_epoch_loss=2.780237\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=2.78207240105\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Epoch[15] Batch [10]#011Speed: 1900.60 samples/sec#011loss=2.782072\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] processed a total of 701 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 549.1340160369873, \"sum\": 549.1340160369873, \"min\": 549.1340160369873}}, \"EndTime\": 1609371916.52257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371915.973385}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1276.33430658 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] #quality_metric: host=algo-1, epoch=15, train loss <loss>=2.78023693778\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_204421f9-2799-48a8-88ea-f7ff0c8a4112-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 13.220071792602539, \"sum\": 13.220071792602539, \"min\": 13.220071792602539}}, \"EndTime\": 1609371916.536206, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371916.522634}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Epoch[16] Batch[0] avg_epoch_loss=2.938158\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=2.93815803528\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Epoch[16] Batch[5] avg_epoch_loss=2.786338\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.78633824984\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:16 INFO 140347361175360] Epoch[16] Batch [5]#011Speed: 1912.81 samples/sec#011loss=2.786338\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 493.502140045166, \"sum\": 493.502140045166, \"min\": 493.502140045166}}, \"EndTime\": 1609371917.029852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371916.53627}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1201.15626772 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.74858698845\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_5133a8f4-7baf-4542-aa53-6d6657ebdf47-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.827852249145508, \"sum\": 9.827852249145508, \"min\": 9.827852249145508}}, \"EndTime\": 1609371917.040317, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371917.029985}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Epoch[17] Batch[0] avg_epoch_loss=2.678823\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=2.67882347107\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Epoch[17] Batch[5] avg_epoch_loss=2.750352\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=2.75035186609\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Epoch[17] Batch [5]#011Speed: 1932.81 samples/sec#011loss=2.750352\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Epoch[17] Batch[10] avg_epoch_loss=2.755221\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=2.76106324196\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Epoch[17] Batch [10]#011Speed: 1908.19 samples/sec#011loss=2.761063\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 541.3119792938232, \"sum\": 541.3119792938232, \"min\": 541.3119792938232}}, \"EndTime\": 1609371917.582589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371917.041216}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1215.33608795 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.7552206733\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Epoch[18] Batch[0] avg_epoch_loss=2.934096\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.93409633636\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Epoch[18] Batch[5] avg_epoch_loss=2.805729\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.80572879314\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:17 INFO 140347361175360] Epoch[18] Batch [5]#011Speed: 1943.74 samples/sec#011loss=2.805729\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 484.57884788513184, \"sum\": 484.57884788513184, \"min\": 484.57884788513184}}, \"EndTime\": 1609371918.067613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371917.582657}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1277.14151998 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.80699768066\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] Epoch[19] Batch[0] avg_epoch_loss=2.815338\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.81533765793\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] Epoch[19] Batch[5] avg_epoch_loss=2.767555\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=2.76755535603\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] Epoch[19] Batch [5]#011Speed: 1927.67 samples/sec#011loss=2.767555\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] Epoch[19] Batch[10] avg_epoch_loss=2.804656\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=2.84917626381\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] Epoch[19] Batch [10]#011Speed: 1870.39 samples/sec#011loss=2.849176\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.2119255065918, \"sum\": 530.2119255065918, \"min\": 530.2119255065918}}, \"EndTime\": 1609371918.598352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371918.067677}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1223.80773011 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #quality_metric: host=algo-1, epoch=19, train loss <loss>=2.80465576865\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] Epoch[20] Batch[0] avg_epoch_loss=2.643365\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.64336490631\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] Epoch[20] Batch[5] avg_epoch_loss=2.748693\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=2.74869330724\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:18 INFO 140347361175360] Epoch[20] Batch [5]#011Speed: 1918.48 samples/sec#011loss=2.748693\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Epoch[20] Batch[10] avg_epoch_loss=2.754928\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=2.76240854263\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Epoch[20] Batch [10]#011Speed: 1865.01 samples/sec#011loss=2.762409\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 562.7219676971436, \"sum\": 562.7219676971436, \"min\": 562.7219676971436}}, \"EndTime\": 1609371919.161444, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371918.598419}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1199.31984374 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #quality_metric: host=algo-1, epoch=20, train loss <loss>=2.75492750515\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Epoch[21] Batch[0] avg_epoch_loss=2.699671\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.69967103004\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Epoch[21] Batch[5] avg_epoch_loss=2.731119\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.73111919562\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Epoch[21] Batch [5]#011Speed: 1796.92 samples/sec#011loss=2.731119\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Epoch[21] Batch[10] avg_epoch_loss=2.684116\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=2.6277121067\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Epoch[21] Batch [10]#011Speed: 1904.43 samples/sec#011loss=2.627712\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 543.0459976196289, \"sum\": 543.0459976196289, \"min\": 543.0459976196289}}, \"EndTime\": 1609371919.704849, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371919.161509}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1222.52319092 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.68411597339\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_7c4d146b-4d8b-4405-b66a-83ad60a800bd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.97018814086914, \"sum\": 9.97018814086914, \"min\": 9.97018814086914}}, \"EndTime\": 1609371919.715334, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371919.704914}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] Epoch[22] Batch[0] avg_epoch_loss=2.693290\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:19 INFO 140347361175360] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=2.69328975677\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Epoch[22] Batch[5] avg_epoch_loss=2.731013\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=2.73101294041\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Epoch[22] Batch [5]#011Speed: 1950.48 samples/sec#011loss=2.731013\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 489.6080493927002, \"sum\": 489.6080493927002, \"min\": 489.6080493927002}}, \"EndTime\": 1609371920.205071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371919.715385}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1300.68474247 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #quality_metric: host=algo-1, epoch=22, train loss <loss>=2.73729453087\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Epoch[23] Batch[0] avg_epoch_loss=2.717220\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.71721959114\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Epoch[23] Batch[5] avg_epoch_loss=2.726520\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.72652014097\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Epoch[23] Batch [5]#011Speed: 1771.40 samples/sec#011loss=2.726520\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Epoch[23] Batch[10] avg_epoch_loss=2.682755\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=2.63023695946\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Epoch[23] Batch [10]#011Speed: 1624.95 samples/sec#011loss=2.630237\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 575.7198333740234, \"sum\": 575.7198333740234, \"min\": 575.7198333740234}}, \"EndTime\": 1609371920.781581, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371920.205149}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1113.20667837 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #quality_metric: host=algo-1, epoch=23, train loss <loss>=2.68275505846\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_b389017b-5459-4f5e-b288-8f70b24176f8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.016918182373047, \"sum\": 10.016918182373047, \"min\": 10.016918182373047}}, \"EndTime\": 1609371920.792075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371920.781645}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] Epoch[24] Batch[0] avg_epoch_loss=2.757968\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:20 INFO 140347361175360] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.75796818733\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] Epoch[24] Batch[5] avg_epoch_loss=2.655592\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=2.65559152762\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] Epoch[24] Batch [5]#011Speed: 1904.59 samples/sec#011loss=2.655592\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 499.42898750305176, \"sum\": 499.42898750305176, \"min\": 499.42898750305176}}, \"EndTime\": 1609371921.291618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371920.792132}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1175.09993036 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #quality_metric: host=algo-1, epoch=24, train loss <loss>=2.69962821007\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] Epoch[25] Batch[0] avg_epoch_loss=2.835427\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.83542728424\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] Epoch[25] Batch[5] avg_epoch_loss=2.723081\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.72308079402\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] Epoch[25] Batch [5]#011Speed: 1894.70 samples/sec#011loss=2.723081\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] processed a total of 582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 489.64500427246094, \"sum\": 489.64500427246094, \"min\": 489.64500427246094}}, \"EndTime\": 1609371921.781736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371921.291686}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1188.37840819 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #quality_metric: host=algo-1, epoch=25, train loss <loss>=2.70330247879\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] Epoch[26] Batch[0] avg_epoch_loss=2.718010\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:21 INFO 140347361175360] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.71801018715\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] Epoch[26] Batch[5] avg_epoch_loss=2.747366\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.74736646811\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] Epoch[26] Batch [5]#011Speed: 1933.18 samples/sec#011loss=2.747366\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 493.1530952453613, \"sum\": 493.1530952453613, \"min\": 493.1530952453613}}, \"EndTime\": 1609371922.275399, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371921.781802}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1287.3823928 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.70804908276\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] Epoch[27] Batch[0] avg_epoch_loss=2.711432\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.71143245697\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] Epoch[27] Batch[5] avg_epoch_loss=2.699717\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=2.6997166872\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] Epoch[27] Batch [5]#011Speed: 1925.06 samples/sec#011loss=2.699717\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] Epoch[27] Batch[10] avg_epoch_loss=2.698211\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=2.69640460014\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] Epoch[27] Batch [10]#011Speed: 1858.88 samples/sec#011loss=2.696405\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 524.4369506835938, \"sum\": 524.4369506835938, \"min\": 524.4369506835938}}, \"EndTime\": 1609371922.800336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371922.275467}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1267.78740427 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] #quality_metric: host=algo-1, epoch=27, train loss <loss>=2.69821119308\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:22 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Epoch[28] Batch[0] avg_epoch_loss=2.666051\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.66605138779\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Epoch[28] Batch[5] avg_epoch_loss=2.706308\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.70630824566\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Epoch[28] Batch [5]#011Speed: 1940.43 samples/sec#011loss=2.706308\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 515.6641006469727, \"sum\": 515.6641006469727, \"min\": 515.6641006469727}}, \"EndTime\": 1609371923.316366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371922.800402}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1207.905078 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.67331805229\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_57f57458-8857-42f1-91cf-97c2884c50d0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.645938873291016, \"sum\": 9.645938873291016, \"min\": 9.645938873291016}}, \"EndTime\": 1609371923.326653, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371923.316435}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Epoch[29] Batch[0] avg_epoch_loss=2.616583\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.61658287048\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Epoch[29] Batch[5] avg_epoch_loss=2.651839\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.65183949471\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Epoch[29] Batch [5]#011Speed: 1931.32 samples/sec#011loss=2.651839\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Epoch[29] Batch[10] avg_epoch_loss=2.685613\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=2.72614116669\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] Epoch[29] Batch [10]#011Speed: 1913.21 samples/sec#011loss=2.726141\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 519.7849273681641, \"sum\": 519.7849273681641, \"min\": 519.7849273681641}}, \"EndTime\": 1609371923.84656, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371923.326712}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1254.10610175 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.68561298197\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:23 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] Epoch[30] Batch[0] avg_epoch_loss=2.624546\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.62454628944\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] Epoch[30] Batch[5] avg_epoch_loss=2.703445\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.70344527562\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] Epoch[30] Batch [5]#011Speed: 1917.91 samples/sec#011loss=2.703445\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 494.12012100219727, \"sum\": 494.12012100219727, \"min\": 494.12012100219727}}, \"EndTime\": 1609371924.341169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371923.846629}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1286.81913033 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.68151500225\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] Epoch[31] Batch[0] avg_epoch_loss=2.714488\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.71448802948\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] Epoch[31] Batch[5] avg_epoch_loss=2.663449\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.66344877084\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] Epoch[31] Batch [5]#011Speed: 1974.71 samples/sec#011loss=2.663449\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 509.7179412841797, \"sum\": 509.7179412841797, \"min\": 509.7179412841797}}, \"EndTime\": 1609371924.851361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371924.341254}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1192.59677146 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.65372543335\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:24 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_4afd92da-0b39-435e-84c6-8c052bd80c2c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.692907333374023, \"sum\": 9.692907333374023, \"min\": 9.692907333374023}}, \"EndTime\": 1609371924.861605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371924.851424}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Epoch[32] Batch[0] avg_epoch_loss=2.610001\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.61000108719\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Epoch[32] Batch[5] avg_epoch_loss=2.620686\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.62068605423\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Epoch[32] Batch [5]#011Speed: 1861.59 samples/sec#011loss=2.620686\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 496.07300758361816, \"sum\": 496.07300758361816, \"min\": 496.07300758361816}}, \"EndTime\": 1609371925.357785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371924.861657}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1227.41181632 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.63437871933\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_da85607e-24a5-4ff1-8a62-ed70e3f44fff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.649991989135742, \"sum\": 9.649991989135742, \"min\": 9.649991989135742}}, \"EndTime\": 1609371925.367991, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371925.357849}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Epoch[33] Batch[0] avg_epoch_loss=2.618042\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.61804175377\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Epoch[33] Batch[5] avg_epoch_loss=2.577211\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.5772105058\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Epoch[33] Batch [5]#011Speed: 1631.93 samples/sec#011loss=2.577211\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Epoch[33] Batch[10] avg_epoch_loss=2.606878\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.64247965813\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Epoch[33] Batch [10]#011Speed: 1698.01 samples/sec#011loss=2.642480\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 571.321964263916, \"sum\": 571.321964263916, \"min\": 571.321964263916}}, \"EndTime\": 1609371925.939422, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371925.368039}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1170.7427427 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.60687830231\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:25 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_f75278fb-b835-4f81-b1a7-17a3738b7fb8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.32400131225586, \"sum\": 10.32400131225586, \"min\": 10.32400131225586}}, \"EndTime\": 1609371925.950282, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371925.939494}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] Epoch[34] Batch[0] avg_epoch_loss=2.487978\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.48797798157\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] Epoch[34] Batch[5] avg_epoch_loss=2.698409\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.69840872288\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] Epoch[34] Batch [5]#011Speed: 1696.60 samples/sec#011loss=2.698409\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 518.6018943786621, \"sum\": 518.6018943786621, \"min\": 518.6018943786621}}, \"EndTime\": 1609371926.468997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371925.950341}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1185.65453309 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.63344237804\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] Epoch[35] Batch[0] avg_epoch_loss=2.564069\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.56406855583\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] Epoch[35] Batch[5] avg_epoch_loss=2.607785\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.60778538386\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] Epoch[35] Batch [5]#011Speed: 1984.35 samples/sec#011loss=2.607785\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] Epoch[35] Batch[10] avg_epoch_loss=2.654025\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=2.70951161385\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] Epoch[35] Batch [10]#011Speed: 1951.80 samples/sec#011loss=2.709512\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 528.6669731140137, \"sum\": 528.6669731140137, \"min\": 528.6669731140137}}, \"EndTime\": 1609371926.998103, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371926.469066}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1229.30069624 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.65402457931\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:26 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] Epoch[36] Batch[0] avg_epoch_loss=2.488608\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.48860836029\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] Epoch[36] Batch[5] avg_epoch_loss=2.590081\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.59008073807\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] Epoch[36] Batch [5]#011Speed: 1775.64 samples/sec#011loss=2.590081\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] Epoch[36] Batch[10] avg_epoch_loss=2.615906\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=2.6468957901\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] Epoch[36] Batch [10]#011Speed: 1879.64 samples/sec#011loss=2.646896\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 545.5918312072754, \"sum\": 545.5918312072754, \"min\": 545.5918312072754}}, \"EndTime\": 1609371927.544123, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371926.998164}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1200.332897 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.61590576172\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] Epoch[37] Batch[0] avg_epoch_loss=2.715277\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.71527719498\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] Epoch[37] Batch[5] avg_epoch_loss=2.647862\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.6478616794\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:27 INFO 140347361175360] Epoch[37] Batch [5]#011Speed: 1878.73 samples/sec#011loss=2.647862\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 482.8810691833496, \"sum\": 482.8810691833496, \"min\": 482.8810691833496}}, \"EndTime\": 1609371928.027374, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371927.544184}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1256.79133239 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.60582580566\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_910ca115-706c-4277-8252-72a83d4c0835-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 13.283967971801758, \"sum\": 13.283967971801758, \"min\": 13.283967971801758}}, \"EndTime\": 1609371928.041112, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371928.027438}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] Epoch[38] Batch[0] avg_epoch_loss=2.523992\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.52399230003\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] Epoch[38] Batch[5] avg_epoch_loss=2.572136\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.57213552793\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] Epoch[38] Batch [5]#011Speed: 1772.44 samples/sec#011loss=2.572136\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 549.6549606323242, \"sum\": 549.6549606323242, \"min\": 549.6549606323242}}, \"EndTime\": 1609371928.590882, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371928.041171}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1136.87429825 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.56556129456\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_fbb9a1a5-4022-401d-9e20-3fcbdb9a1d2d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 13.247966766357422, \"sum\": 13.247966766357422, \"min\": 13.247966766357422}}, \"EndTime\": 1609371928.604583, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371928.590947}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] Epoch[39] Batch[0] avg_epoch_loss=2.682913\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.68291330338\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] Epoch[39] Batch[5] avg_epoch_loss=2.574612\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.57461158435\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:28 INFO 140347361175360] Epoch[39] Batch [5]#011Speed: 1929.93 samples/sec#011loss=2.574612\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 493.17407608032227, \"sum\": 493.17407608032227, \"min\": 493.17407608032227}}, \"EndTime\": 1609371929.09788, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371928.604645}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1283.25755347 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #quality_metric: host=algo-1, epoch=39, train loss <loss>=2.56135237217\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_4d926481-d69f-4fb6-a9db-94bf89ca61c5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.756088256835938, \"sum\": 9.756088256835938, \"min\": 9.756088256835938}}, \"EndTime\": 1609371929.108222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371929.097947}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] Epoch[40] Batch[0] avg_epoch_loss=2.523742\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.52374243736\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] Epoch[40] Batch[5] avg_epoch_loss=2.573321\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.57332086563\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] Epoch[40] Batch [5]#011Speed: 1790.76 samples/sec#011loss=2.573321\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] Epoch[40] Batch[10] avg_epoch_loss=2.606806\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=2.64698753357\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] Epoch[40] Batch [10]#011Speed: 1901.50 samples/sec#011loss=2.646988\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 552.5381565093994, \"sum\": 552.5381565093994, \"min\": 552.5381565093994}}, \"EndTime\": 1609371929.660884, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371929.108287}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1190.65888038 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.60680571469\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] Epoch[41] Batch[0] avg_epoch_loss=2.572523\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:29 INFO 140347361175360] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.57252335548\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[41] Batch[5] avg_epoch_loss=2.579031\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.57903134823\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[41] Batch [5]#011Speed: 1917.74 samples/sec#011loss=2.579031\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[41] Batch[10] avg_epoch_loss=2.518430\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=2.44570884705\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[41] Batch [10]#011Speed: 1952.64 samples/sec#011loss=2.445709\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.0449695587158, \"sum\": 521.0449695587158, \"min\": 521.0449695587158}}, \"EndTime\": 1609371930.182449, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371929.660947}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1268.29809113 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.51843021133\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_9362b499-f69b-438e-af20-fcbe5ed02b20-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.763002395629883, \"sum\": 9.763002395629883, \"min\": 9.763002395629883}}, \"EndTime\": 1609371930.192963, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371930.182508}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[42] Batch[0] avg_epoch_loss=2.703695\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.7036948204\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[42] Batch[5] avg_epoch_loss=2.548765\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.5487651825\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[42] Batch [5]#011Speed: 2001.98 samples/sec#011loss=2.548765\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[42] Batch[10] avg_epoch_loss=2.551367\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=2.55448818207\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[42] Batch [10]#011Speed: 1999.17 samples/sec#011loss=2.554488\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 512.1231079101562, \"sum\": 512.1231079101562, \"min\": 512.1231079101562}}, \"EndTime\": 1609371930.705191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371930.193016}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1319.75549318 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.55136654594\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] Epoch[43] Batch[0] avg_epoch_loss=2.526402\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:30 INFO 140347361175360] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.52640175819\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Epoch[43] Batch[5] avg_epoch_loss=2.536394\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.53639380137\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Epoch[43] Batch [5]#011Speed: 1885.80 samples/sec#011loss=2.536394\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 510.03408432006836, \"sum\": 510.03408432006836, \"min\": 510.03408432006836}}, \"EndTime\": 1609371931.215699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371930.705254}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1215.37607477 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.52809224129\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Epoch[44] Batch[0] avg_epoch_loss=2.599443\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.59944343567\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Epoch[44] Batch[5] avg_epoch_loss=2.508652\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.50865248839\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Epoch[44] Batch [5]#011Speed: 1831.57 samples/sec#011loss=2.508652\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Epoch[44] Batch[10] avg_epoch_loss=2.483451\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=2.45320901871\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Epoch[44] Batch [10]#011Speed: 1878.43 samples/sec#011loss=2.453209\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 552.156925201416, \"sum\": 552.156925201416, \"min\": 552.156925201416}}, \"EndTime\": 1609371931.768268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371931.215763}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1220.44403537 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.48345091126\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_652bff20-3190-4c09-a21d-b7cd42a18a04-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.128974914550781, \"sum\": 10.128974914550781, \"min\": 10.128974914550781}}, \"EndTime\": 1609371931.778901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371931.768336}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] Epoch[45] Batch[0] avg_epoch_loss=2.373453\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:31 INFO 140347361175360] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.37345290184\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[45] Batch[5] avg_epoch_loss=2.520841\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.5208414793\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[45] Batch [5]#011Speed: 1880.50 samples/sec#011loss=2.520841\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[45] Batch[10] avg_epoch_loss=2.501577\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=2.47845907211\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[45] Batch [10]#011Speed: 1997.25 samples/sec#011loss=2.478459\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 513.8368606567383, \"sum\": 513.8368606567383, \"min\": 513.8368606567383}}, \"EndTime\": 1609371932.292841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371931.778953}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1256.98556948 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.50157674876\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[46] Batch[0] avg_epoch_loss=2.639502\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.63950228691\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[46] Batch[5] avg_epoch_loss=2.545992\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=2.54599153996\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[46] Batch [5]#011Speed: 2015.98 samples/sec#011loss=2.545992\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[46] Batch[10] avg_epoch_loss=2.548751\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=2.55206203461\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] Epoch[46] Batch [10]#011Speed: 1977.19 samples/sec#011loss=2.552062\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 517.9860591888428, \"sum\": 517.9860591888428, \"min\": 517.9860591888428}}, \"EndTime\": 1609371932.811305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371932.292902}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1256.56964168 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] #quality_metric: host=algo-1, epoch=46, train loss <loss>=2.54875085571\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:32 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[47] Batch[0] avg_epoch_loss=2.639234\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.63923358917\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[47] Batch[5] avg_epoch_loss=2.542840\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.54284044107\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[47] Batch [5]#011Speed: 1966.34 samples/sec#011loss=2.542840\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[47] Batch[10] avg_epoch_loss=2.502938\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=2.45505466461\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[47] Batch [10]#011Speed: 1937.62 samples/sec#011loss=2.455055\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 526.8521308898926, \"sum\": 526.8521308898926, \"min\": 526.8521308898926}}, \"EndTime\": 1609371933.338621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371932.811367}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1263.88417212 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.50293781541\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[48] Batch[0] avg_epoch_loss=2.759330\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=2.75933003426\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[48] Batch[5] avg_epoch_loss=2.540830\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.54082993666\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[48] Batch [5]#011Speed: 1943.88 samples/sec#011loss=2.540830\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[48] Batch[10] avg_epoch_loss=2.532339\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=2.52214989662\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] Epoch[48] Batch [10]#011Speed: 1949.70 samples/sec#011loss=2.522150\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 526.777982711792, \"sum\": 526.777982711792, \"min\": 526.777982711792}}, \"EndTime\": 1609371933.865849, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371933.338687}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1264.06146979 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.53233900937\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:33 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] Epoch[49] Batch[0] avg_epoch_loss=2.612334\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.61233401299\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] Epoch[49] Batch[5] avg_epoch_loss=2.566894\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.56689361731\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] Epoch[49] Batch [5]#011Speed: 1896.95 samples/sec#011loss=2.566894\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] Epoch[49] Batch[10] avg_epoch_loss=2.521365\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=2.46673121452\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] Epoch[49] Batch [10]#011Speed: 1710.17 samples/sec#011loss=2.466731\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 574.9130249023438, \"sum\": 574.9130249023438, \"min\": 574.9130249023438}}, \"EndTime\": 1609371934.441126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371933.865914}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1161.7168761 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.52136525241\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] Epoch[50] Batch[0] avg_epoch_loss=2.645026\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.64502596855\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] Epoch[50] Batch[5] avg_epoch_loss=2.534051\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=2.53405050437\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] Epoch[50] Batch [5]#011Speed: 1911.60 samples/sec#011loss=2.534051\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 495.0680732727051, \"sum\": 495.0680732727051, \"min\": 495.0680732727051}}, \"EndTime\": 1609371934.936748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371934.441189}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1274.31750977 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] #quality_metric: host=algo-1, epoch=50, train loss <loss>=2.49079146385\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:34 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] Epoch[51] Batch[0] avg_epoch_loss=2.417447\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.41744732857\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] Epoch[51] Batch[5] avg_epoch_loss=2.508608\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.50860822201\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] Epoch[51] Batch [5]#011Speed: 1891.11 samples/sec#011loss=2.508608\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] Epoch[51] Batch[10] avg_epoch_loss=2.532668\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=2.56154003143\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] Epoch[51] Batch [10]#011Speed: 1810.56 samples/sec#011loss=2.561540\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 551.1910915374756, \"sum\": 551.1910915374756, \"min\": 551.1910915374756}}, \"EndTime\": 1609371935.488458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371934.936814}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1202.62074304 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.53266813538\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] Epoch[52] Batch[0] avg_epoch_loss=2.382865\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=2.38286495209\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] Epoch[52] Batch[5] avg_epoch_loss=2.455666\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.45566562812\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] Epoch[52] Batch [5]#011Speed: 1876.38 samples/sec#011loss=2.455666\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 508.9139938354492, \"sum\": 508.9139938354492, \"min\": 508.9139938354492}}, \"EndTime\": 1609371935.997843, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371935.488529}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1214.0639125 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.43954000473\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:35 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_89f30999-119e-426c-a3a0-cb66b527762a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.525941848754883, \"sum\": 10.525941848754883, \"min\": 10.525941848754883}}, \"EndTime\": 1609371936.008982, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371935.997923}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Epoch[53] Batch[0] avg_epoch_loss=2.455197\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.45519733429\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Epoch[53] Batch[5] avg_epoch_loss=2.499317\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.49931653341\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Epoch[53] Batch [5]#011Speed: 1896.08 samples/sec#011loss=2.499317\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Epoch[53] Batch[10] avg_epoch_loss=2.505871\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=2.51373724937\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Epoch[53] Batch [10]#011Speed: 1915.11 samples/sec#011loss=2.513737\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.8501720428467, \"sum\": 530.8501720428467, \"min\": 530.8501720428467}}, \"EndTime\": 1609371936.53997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371936.009056}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1227.96700678 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.5058714043\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Epoch[54] Batch[0] avg_epoch_loss=2.492414\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.49241375923\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Epoch[54] Batch[5] avg_epoch_loss=2.455381\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.45538119475\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:36 INFO 140347361175360] Epoch[54] Batch [5]#011Speed: 1935.93 samples/sec#011loss=2.455381\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 485.3360652923584, \"sum\": 485.3360652923584, \"min\": 485.3360652923584}}, \"EndTime\": 1609371937.025838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371936.540041}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1301.93592381 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.50633692741\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] Epoch[55] Batch[0] avg_epoch_loss=2.546512\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.54651236534\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] Epoch[55] Batch[5] avg_epoch_loss=2.483929\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.48392899831\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] Epoch[55] Batch [5]#011Speed: 1934.74 samples/sec#011loss=2.483929\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 491.1339282989502, \"sum\": 491.1339282989502, \"min\": 491.1339282989502}}, \"EndTime\": 1609371937.517479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371937.025903}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1282.49244311 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.42849183083\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_e2c16db2-948d-44ae-ac73-0324c9f96a3e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.816884994506836, \"sum\": 9.816884994506836, \"min\": 9.816884994506836}}, \"EndTime\": 1609371937.527834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371937.517539}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] Epoch[56] Batch[0] avg_epoch_loss=2.493284\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.49328398705\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] Epoch[56] Batch[5] avg_epoch_loss=2.487491\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.48749113083\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:37 INFO 140347361175360] Epoch[56] Batch [5]#011Speed: 1948.36 samples/sec#011loss=2.487491\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 478.93691062927246, \"sum\": 478.93691062927246, \"min\": 478.93691062927246}}, \"EndTime\": 1609371938.006884, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371937.527887}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1325.59840692 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.45968811512\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] Epoch[57] Batch[0] avg_epoch_loss=2.440010\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=2.44000983238\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] Epoch[57] Batch[5] avg_epoch_loss=2.433945\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=2.43394521872\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] Epoch[57] Batch [5]#011Speed: 1902.96 samples/sec#011loss=2.433945\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 484.50589179992676, \"sum\": 484.50589179992676, \"min\": 484.50589179992676}}, \"EndTime\": 1609371938.491862, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371938.006942}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1225.6975498 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #quality_metric: host=algo-1, epoch=57, train loss <loss>=2.46756036282\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] Epoch[58] Batch[0] avg_epoch_loss=2.442257\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.44225668907\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] Epoch[58] Batch[5] avg_epoch_loss=2.480114\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=2.48011366526\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] Epoch[58] Batch [5]#011Speed: 1980.64 samples/sec#011loss=2.480114\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 489.0577793121338, \"sum\": 489.0577793121338, \"min\": 489.0577793121338}}, \"EndTime\": 1609371938.981362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371938.491945}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1234.80034996 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] #quality_metric: host=algo-1, epoch=58, train loss <loss>=2.52590658665\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:38 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] Epoch[59] Batch[0] avg_epoch_loss=2.487489\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=2.4874894619\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] Epoch[59] Batch[5] avg_epoch_loss=2.446011\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=2.44601122538\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] Epoch[59] Batch [5]#011Speed: 1974.86 samples/sec#011loss=2.446011\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] Epoch[59] Batch[10] avg_epoch_loss=2.435147\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=2.4221095562\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] Epoch[59] Batch [10]#011Speed: 1887.21 samples/sec#011loss=2.422110\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 521.3568210601807, \"sum\": 521.3568210601807, \"min\": 521.3568210601807}}, \"EndTime\": 1609371939.503174, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371938.981421}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1309.79710124 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] #quality_metric: host=algo-1, epoch=59, train loss <loss>=2.4351468303\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] Epoch[60] Batch[0] avg_epoch_loss=2.460422\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=2.46042180061\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] Epoch[60] Batch[5] avg_epoch_loss=2.447200\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.44719950358\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:39 INFO 140347361175360] Epoch[60] Batch [5]#011Speed: 1733.01 samples/sec#011loss=2.447200\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 505.7530403137207, \"sum\": 505.7530403137207, \"min\": 505.7530403137207}}, \"EndTime\": 1609371940.009413, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371939.503239}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1245.42125398 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.45067322254\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Epoch[61] Batch[0] avg_epoch_loss=2.563582\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.56358194351\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Epoch[61] Batch[5] avg_epoch_loss=2.448557\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.44855721792\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Epoch[61] Batch [5]#011Speed: 1765.95 samples/sec#011loss=2.448557\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Epoch[61] Batch[10] avg_epoch_loss=2.414206\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=2.37298536301\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Epoch[61] Batch [10]#011Speed: 1808.54 samples/sec#011loss=2.372985\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 551.0449409484863, \"sum\": 551.0449409484863, \"min\": 551.0449409484863}}, \"EndTime\": 1609371940.561016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371940.009481}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1222.92384328 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.41420637478\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_32270e99-a259-4fb5-b6f4-a12116dfdf1c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.72294807434082, \"sum\": 9.72294807434082, \"min\": 9.72294807434082}}, \"EndTime\": 1609371940.571299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371940.561073}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Epoch[62] Batch[0] avg_epoch_loss=2.423632\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.42363190651\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Epoch[62] Batch[5] avg_epoch_loss=2.447578\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.44757803281\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:40 INFO 140347361175360] Epoch[62] Batch [5]#011Speed: 1779.46 samples/sec#011loss=2.447578\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.8558731079102, \"sum\": 527.8558731079102, \"min\": 527.8558731079102}}, \"EndTime\": 1609371941.099271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371940.571356}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1183.81075213 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.44599745274\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] Epoch[63] Batch[0] avg_epoch_loss=2.459874\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.45987415314\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] Epoch[63] Batch[5] avg_epoch_loss=2.450018\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.45001832644\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] Epoch[63] Batch [5]#011Speed: 1791.39 samples/sec#011loss=2.450018\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 557.3039054870605, \"sum\": 557.3039054870605, \"min\": 557.3039054870605}}, \"EndTime\": 1609371941.657113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371941.099338}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1110.50124512 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.41838657856\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] Epoch[64] Batch[0] avg_epoch_loss=2.447622\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:41 INFO 140347361175360] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.44762182236\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[64] Batch[5] avg_epoch_loss=2.378607\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.37860731284\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[64] Batch [5]#011Speed: 1948.64 samples/sec#011loss=2.378607\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[64] Batch[10] avg_epoch_loss=2.401643\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=2.42928476334\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[64] Batch [10]#011Speed: 1860.74 samples/sec#011loss=2.429285\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 535.8190536499023, \"sum\": 535.8190536499023, \"min\": 535.8190536499023}}, \"EndTime\": 1609371942.19338, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371941.657181}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1248.33252928 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.40164251761\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_5befa1d3-5491-4781-bba0-a320ff0accaf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 13.098001480102539, \"sum\": 13.098001480102539, \"min\": 13.098001480102539}}, \"EndTime\": 1609371942.206902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371942.193446}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[65] Batch[0] avg_epoch_loss=2.530072\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=2.5300719738\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[65] Batch[5] avg_epoch_loss=2.539130\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.53912961483\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[65] Batch [5]#011Speed: 1859.84 samples/sec#011loss=2.539130\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[65] Batch[10] avg_epoch_loss=2.440070\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=2.32119774818\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[65] Batch [10]#011Speed: 1923.67 samples/sec#011loss=2.321198\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 530.0648212432861, \"sum\": 530.0648212432861, \"min\": 530.0648212432861}}, \"EndTime\": 1609371942.737086, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371942.206964}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1233.57388762 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.44006967545\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] Epoch[66] Batch[0] avg_epoch_loss=2.514134\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:42 INFO 140347361175360] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=2.51413440704\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[66] Batch[5] avg_epoch_loss=2.577067\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=2.57706725597\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[66] Batch [5]#011Speed: 1871.62 samples/sec#011loss=2.577067\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[66] Batch[10] avg_epoch_loss=2.533445\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=2.48109927177\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[66] Batch [10]#011Speed: 1905.47 samples/sec#011loss=2.481099\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.8830528259277, \"sum\": 527.8830528259277, \"min\": 527.8830528259277}}, \"EndTime\": 1609371943.265396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371942.737156}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1272.78044163 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #quality_metric: host=algo-1, epoch=66, train loss <loss>=2.53344544497\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[67] Batch[0] avg_epoch_loss=2.459528\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=2.45952773094\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[67] Batch[5] avg_epoch_loss=2.458436\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=2.45843601227\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[67] Batch [5]#011Speed: 1800.50 samples/sec#011loss=2.458436\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[67] Batch[10] avg_epoch_loss=2.416627\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=2.36645627022\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[67] Batch [10]#011Speed: 1909.59 samples/sec#011loss=2.366456\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 537.0519161224365, \"sum\": 537.0519161224365, \"min\": 537.0519161224365}}, \"EndTime\": 1609371943.802871, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371943.26546}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1269.6636255 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #quality_metric: host=algo-1, epoch=67, train loss <loss>=2.41662703861\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] Epoch[68] Batch[0] avg_epoch_loss=2.319743\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:43 INFO 140347361175360] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=2.3197426796\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] Epoch[68] Batch[5] avg_epoch_loss=2.413533\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=2.41353269418\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] Epoch[68] Batch [5]#011Speed: 1911.30 samples/sec#011loss=2.413533\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 480.9119701385498, \"sum\": 480.9119701385498, \"min\": 480.9119701385498}}, \"EndTime\": 1609371944.284212, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371943.802938}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1322.1799322 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #quality_metric: host=algo-1, epoch=68, train loss <loss>=2.39195392132\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_8a4ec844-4e28-4ce3-9138-ca044fa7bc46-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.554147720336914, \"sum\": 9.554147720336914, \"min\": 9.554147720336914}}, \"EndTime\": 1609371944.294266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371944.284292}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] Epoch[69] Batch[0] avg_epoch_loss=2.431217\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.43121743202\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] Epoch[69] Batch[5] avg_epoch_loss=2.462062\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.46206176281\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] Epoch[69] Batch [5]#011Speed: 1887.66 samples/sec#011loss=2.462062\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 490.01502990722656, \"sum\": 490.01502990722656, \"min\": 490.01502990722656}}, \"EndTime\": 1609371944.784381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371944.294314}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1299.72108482 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.43845627308\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] Epoch[70] Batch[0] avg_epoch_loss=2.492061\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:44 INFO 140347361175360] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=2.49206089973\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[70] Batch[5] avg_epoch_loss=2.384449\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=2.38444852829\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[70] Batch [5]#011Speed: 1923.11 samples/sec#011loss=2.384449\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[70] Batch[10] avg_epoch_loss=2.389060\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=2.39459486008\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[70] Batch [10]#011Speed: 1955.58 samples/sec#011loss=2.394595\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 515.2599811553955, \"sum\": 515.2599811553955, \"min\": 515.2599811553955}}, \"EndTime\": 1609371945.300186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371944.784441}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1243.80024797 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #quality_metric: host=algo-1, epoch=70, train loss <loss>=2.38906049728\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_c6c6b8cf-b470-4b59-a378-35848868d70a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.433984756469727, \"sum\": 9.433984756469727, \"min\": 9.433984756469727}}, \"EndTime\": 1609371945.310122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371945.300252}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[71] Batch[0] avg_epoch_loss=2.474997\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=2.47499704361\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[71] Batch[5] avg_epoch_loss=2.458201\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=2.4582012097\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[71] Batch [5]#011Speed: 1879.15 samples/sec#011loss=2.458201\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[71] Batch[10] avg_epoch_loss=2.460233\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=2.46267056465\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] Epoch[71] Batch [10]#011Speed: 1928.77 samples/sec#011loss=2.462671\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] processed a total of 716 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 571.1779594421387, \"sum\": 571.1779594421387, \"min\": 571.1779594421387}}, \"EndTime\": 1609371945.881416, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371945.310183}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1253.33684348 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] #quality_metric: host=algo-1, epoch=71, train loss <loss>=2.46112720172\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:45 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] Epoch[72] Batch[0] avg_epoch_loss=2.552615\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=2.55261492729\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] Epoch[72] Batch[5] avg_epoch_loss=2.429008\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=2.42900768916\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] Epoch[72] Batch [5]#011Speed: 1679.83 samples/sec#011loss=2.429008\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] Epoch[72] Batch[10] avg_epoch_loss=2.412664\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=2.39305100441\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] Epoch[72] Batch [10]#011Speed: 1728.65 samples/sec#011loss=2.393051\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 637.8288269042969, \"sum\": 637.8288269042969, \"min\": 637.8288269042969}}, \"EndTime\": 1609371946.519739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371945.881481}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1069.08615133 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] #quality_metric: host=algo-1, epoch=72, train loss <loss>=2.41266374155\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] Epoch[73] Batch[0] avg_epoch_loss=2.500670\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.50067019463\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] Epoch[73] Batch[5] avg_epoch_loss=2.421522\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=2.42152166367\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:46 INFO 140347361175360] Epoch[73] Batch [5]#011Speed: 1911.60 samples/sec#011loss=2.421522\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Epoch[73] Batch[10] avg_epoch_loss=2.372189\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=2.31299052238\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Epoch[73] Batch [10]#011Speed: 1916.98 samples/sec#011loss=2.312991\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 569.3728923797607, \"sum\": 569.3728923797607, \"min\": 569.3728923797607}}, \"EndTime\": 1609371947.089476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371946.519805}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1146.67030848 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #quality_metric: host=algo-1, epoch=73, train loss <loss>=2.37218932672\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_130ed570-918a-460e-8d69-4e9f25508fe5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.271072387695312, \"sum\": 10.271072387695312, \"min\": 10.271072387695312}}, \"EndTime\": 1609371947.100242, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371947.089546}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Epoch[74] Batch[0] avg_epoch_loss=2.303805\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=2.30380535126\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Epoch[74] Batch[5] avg_epoch_loss=2.404309\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=2.40430899461\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Epoch[74] Batch [5]#011Speed: 1925.35 samples/sec#011loss=2.404309\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 481.55999183654785, \"sum\": 481.55999183654785, \"min\": 481.55999183654785}}, \"EndTime\": 1609371947.581916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371947.100304}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1266.47700143 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #quality_metric: host=algo-1, epoch=74, train loss <loss>=2.33092191219\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/state_ce450d0b-4781-4676-af12-ead95a36d781-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.64498519897461, \"sum\": 9.64498519897461, \"min\": 9.64498519897461}}, \"EndTime\": 1609371947.592121, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371947.581979}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Epoch[75] Batch[0] avg_epoch_loss=2.403890\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=2.40388989449\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Epoch[75] Batch[5] avg_epoch_loss=2.395344\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=2.39534350236\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:47 INFO 140347361175360] Epoch[75] Batch [5]#011Speed: 1957.57 samples/sec#011loss=2.395344\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 499.4521141052246, \"sum\": 499.4521141052246, \"min\": 499.4521141052246}}, \"EndTime\": 1609371948.091675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371947.59217}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1275.1704937 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #quality_metric: host=algo-1, epoch=75, train loss <loss>=2.40614523888\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] Epoch[76] Batch[0] avg_epoch_loss=2.464837\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=2.46483659744\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] Epoch[76] Batch[5] avg_epoch_loss=2.415059\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=2.41505908966\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] Epoch[76] Batch [5]#011Speed: 1976.33 samples/sec#011loss=2.415059\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 488.2850646972656, \"sum\": 488.2850646972656, \"min\": 488.2850646972656}}, \"EndTime\": 1609371948.580397, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371948.091734}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1281.79139137 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #quality_metric: host=algo-1, epoch=76, train loss <loss>=2.40057089329\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] Epoch[77] Batch[0] avg_epoch_loss=2.388438\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.38843846321\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] Epoch[77] Batch[5] avg_epoch_loss=2.390566\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.39056638877\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:48 INFO 140347361175360] Epoch[77] Batch [5]#011Speed: 1956.26 samples/sec#011loss=2.390566\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] Epoch[77] Batch[10] avg_epoch_loss=2.350815\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=2.30311293602\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] Epoch[77] Batch [10]#011Speed: 1936.96 samples/sec#011loss=2.303113\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 527.4741649627686, \"sum\": 527.4741649627686, \"min\": 527.4741649627686}}, \"EndTime\": 1609371949.108323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371948.580462}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1252.91428329 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #quality_metric: host=algo-1, epoch=77, train loss <loss>=2.35081481934\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] Epoch[78] Batch[0] avg_epoch_loss=2.414088\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=2.41408801079\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] Epoch[78] Batch[5] avg_epoch_loss=2.397328\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=2.3973282973\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] Epoch[78] Batch [5]#011Speed: 1926.25 samples/sec#011loss=2.397328\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 500.55694580078125, \"sum\": 500.55694580078125, \"min\": 500.55694580078125}}, \"EndTime\": 1609371949.609262, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371949.108388}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1256.09592774 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #quality_metric: host=algo-1, epoch=78, train loss <loss>=2.37873914242\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] Epoch[79] Batch[0] avg_epoch_loss=2.361592\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.36159229279\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] Epoch[79] Batch[5] avg_epoch_loss=2.346169\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=2.34616891543\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:49 INFO 140347361175360] Epoch[79] Batch [5]#011Speed: 1962.01 samples/sec#011loss=2.346169\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] Epoch[79] Batch[10] avg_epoch_loss=2.407835\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=2.48183412552\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] Epoch[79] Batch [10]#011Speed: 1975.57 samples/sec#011loss=2.481834\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 516.442060470581, \"sum\": 516.442060470581, \"min\": 516.442060470581}}, \"EndTime\": 1609371950.126343, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371949.609417}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1337.71631551 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #quality_metric: host=algo-1, epoch=79, train loss <loss>=2.40783492002\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] Epoch[80] Batch[0] avg_epoch_loss=2.456124\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=2.45612430573\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] Epoch[80] Batch[5] avg_epoch_loss=2.427195\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=2.42719531059\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] Epoch[80] Batch [5]#011Speed: 1974.37 samples/sec#011loss=2.427195\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] Epoch[80] Batch[10] avg_epoch_loss=2.399877\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=2.36709432602\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] Epoch[80] Batch [10]#011Speed: 1965.22 samples/sec#011loss=2.367094\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 509.2599391937256, \"sum\": 509.2599391937256, \"min\": 509.2599391937256}}, \"EndTime\": 1609371950.636046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371950.126421}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1289.87427219 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #quality_metric: host=algo-1, epoch=80, train loss <loss>=2.39987668124\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] Epoch[81] Batch[0] avg_epoch_loss=2.261890\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:50 INFO 140347361175360] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=2.26188969612\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] Epoch[81] Batch[5] avg_epoch_loss=2.384953\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=2.38495282332\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] Epoch[81] Batch [5]#011Speed: 1973.63 samples/sec#011loss=2.384953\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 540.6970977783203, \"sum\": 540.6970977783203, \"min\": 540.6970977783203}}, \"EndTime\": 1609371951.17711, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371950.636107}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1172.31329888 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #quality_metric: host=algo-1, epoch=81, train loss <loss>=2.40753712654\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] Epoch[82] Batch[0] avg_epoch_loss=2.465127\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=2.46512675285\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] Epoch[82] Batch[5] avg_epoch_loss=2.416384\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=2.41638394197\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] Epoch[82] Batch [5]#011Speed: 1850.30 samples/sec#011loss=2.416384\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 492.63691902160645, \"sum\": 492.63691902160645, \"min\": 492.63691902160645}}, \"EndTime\": 1609371951.670314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371951.177192}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1197.38161492 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #quality_metric: host=algo-1, epoch=82, train loss <loss>=2.3833958149\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] Epoch[83] Batch[0] avg_epoch_loss=2.379336\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:51 INFO 140347361175360] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=2.37933588028\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Epoch[83] Batch[5] avg_epoch_loss=2.427741\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=2.42774109046\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Epoch[83] Batch [5]#011Speed: 1926.11 samples/sec#011loss=2.427741\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 498.2738494873047, \"sum\": 498.2738494873047, \"min\": 498.2738494873047}}, \"EndTime\": 1609371952.169016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371951.670385}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1276.18365604 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #quality_metric: host=algo-1, epoch=83, train loss <loss>=2.40997424126\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Epoch[84] Batch[0] avg_epoch_loss=2.364770\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=2.36476993561\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Epoch[84] Batch[5] avg_epoch_loss=2.386310\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=2.38630986214\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Epoch[84] Batch [5]#011Speed: 1885.87 samples/sec#011loss=2.386310\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 490.00000953674316, \"sum\": 490.00000953674316, \"min\": 490.00000953674316}}, \"EndTime\": 1609371952.659579, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371952.169072}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #throughput_metric: host=algo-1, train throughput=1291.60229658 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #quality_metric: host=algo-1, epoch=84, train loss <loss>=2.3692492485\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Loading parameters from best epoch (74)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 4.760980606079102, \"sum\": 4.760980606079102, \"min\": 4.760980606079102}}, \"EndTime\": 1609371952.664924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371952.659637}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] stopping training now\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Final loss: 2.33092191219 (occurred at epoch 74)\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] #quality_metric: host=algo-1, train final_loss <loss>=2.33092191219\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 WARNING 140347361175360] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 53.84111404418945, \"sum\": 53.84111404418945, \"min\": 53.84111404418945}}, \"EndTime\": 1609371952.719491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371952.664981}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 101.58300399780273, \"sum\": 101.58300399780273, \"min\": 101.58300399780273}}, \"EndTime\": 1609371952.767193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371952.719558}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 4.813909530639648, \"sum\": 4.813909530639648, \"min\": 4.813909530639648}}, \"EndTime\": 1609371952.772107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371952.767256}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:52 INFO 140347361175360] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.03409385681152344, \"sum\": 0.03409385681152344, \"min\": 0.03409385681152344}}, \"EndTime\": 1609371952.772796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371952.772151}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 321.89011573791504, \"sum\": 321.89011573791504, \"min\": 321.89011573791504}}, \"EndTime\": 1609371953.094649, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371952.772853}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, RMSE): 3.50527338853\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, mean_absolute_QuantileLoss): 10.366470506456164\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, mean_wQuantileLoss): 0.01623838359637133\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.1]): 0.010731200387478058\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.2]): 0.017090002314657463\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.3]): 0.019972102210913768\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.4]): 0.021014623070796108\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.5]): 0.01928709123851957\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.6]): 0.01747219805742779\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.7]): 0.017256160970897938\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.8]): 0.012621962896452014\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #test_score (algo-1, wQuantileLoss[0.9]): 0.010700111220199263\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.0162383835964\u001b[0m\n",
      "\u001b[34m[12/30/2020 23:45:53 INFO 140347361175360] #quality_metric: host=algo-1, test RMSE <loss>=3.50527338853\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 46119.08197402954, \"sum\": 46119.08197402954, \"min\": 46119.08197402954}, \"setuptime\": {\"count\": 1, \"max\": 8.363008499145508, \"sum\": 8.363008499145508, \"min\": 8.363008499145508}}, \"EndTime\": 1609371953.103159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1609371953.094713}\n",
      "\u001b[0m\n",
      "\n",
      "2020-12-30 23:46:04 Uploading - Uploading generated training model\n",
      "2020-12-30 23:46:04 Completed - Training job completed\n",
      "Training seconds: 96\n",
      "Billable seconds: 96\n"
     ]
    }
   ],
   "source": [
    "# This step takes around 35 minutes to train the model with m4.xlarge instance\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also deploy a model using the job name. The job name is also available \n",
    "# in the sagemaker console -> Training -> Training Jobs\n",
    "# job_name = 'deepar-biketrain-with-categories-2018-12-21-04-05-44-478'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job name: AAPL-no-categories-2020-12-30-23-42-14-622\n"
     ]
    }
   ],
   "source": [
    "print ('job name: {0}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# Create an endpoint for real-time predictions\n",
    "# SDK 2. parameter name for container: image_uri\n",
    "\n",
    "endpoint_name = sess.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    image_uri=container,\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: AAPL-no-categories-2020-12-30-23-42-14-622\n"
     ]
    }
   ],
   "source": [
    "print ('endpoint name: {0}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next lab, we will use the above endpoint for inference\n",
    "# We will delete the endpoint in the next lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
